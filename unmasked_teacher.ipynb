{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rdW3TqwAnAmd",
        "0Vw4YoAmoOIT",
        "gUev2yWWxXae",
        "OjprQ0e0-XBF",
        "7m_Zs5WFyW0o",
        "7pr6DXYqrKFj",
        "LaFqRHA6rGXE",
        "bFQzNOmdzhOj",
        "xRFD1lbdtmBr",
        "mzC2L48Y_kyP",
        "kQj3j529Rebm",
        "0Sp5wFpwZIAh"
      ],
      "gpuType": "V100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a738cb1d60964faaba7d6f3a5f665f5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_da9e1072e92c48949814f3be812ce521",
              "IPY_MODEL_ca594cbbc0f249d4bbf16682989cd1ed",
              "IPY_MODEL_217a453177ac4216b888ecaac9eea6bf"
            ],
            "layout": "IPY_MODEL_ef14200fb01743dc8a9533df44197507"
          }
        },
        "da9e1072e92c48949814f3be812ce521": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e63022385c514b7cb9cd819ee4f2b402",
            "placeholder": "​",
            "style": "IPY_MODEL_163d7cc6529c46e48be0084dbb336278",
            "value": "Downloading vocab.txt: 100%"
          }
        },
        "ca594cbbc0f249d4bbf16682989cd1ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43f297e1fb17443b92652202a61aaa7d",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_44792c58989b405db23be4771d1db5ca",
            "value": 231508
          }
        },
        "217a453177ac4216b888ecaac9eea6bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_600c16017e994026ba4d30956fa1740b",
            "placeholder": "​",
            "style": "IPY_MODEL_6f7f1dc22c774645b347c157a553f404",
            "value": " 226k/226k [00:00&lt;00:00, 253kB/s]"
          }
        },
        "ef14200fb01743dc8a9533df44197507": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e63022385c514b7cb9cd819ee4f2b402": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "163d7cc6529c46e48be0084dbb336278": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43f297e1fb17443b92652202a61aaa7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44792c58989b405db23be4771d1db5ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "600c16017e994026ba4d30956fa1740b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f7f1dc22c774645b347c157a553f404": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ced496820a1a4803b086b626f6b9e439": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0449838b4be4c529a2a3fd29d0f174b",
              "IPY_MODEL_4b786ede155a4bd393f04698b08cb8ae",
              "IPY_MODEL_8637da21baa342dbbe09a5b10e84cda3"
            ],
            "layout": "IPY_MODEL_13f3b292cf1d4f44ac24f116814c01b7"
          }
        },
        "c0449838b4be4c529a2a3fd29d0f174b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3a1ebf6a3eb4aa89368b6e46ae04bd0",
            "placeholder": "​",
            "style": "IPY_MODEL_2dddb634372d42aaab1a6aa4207ba24a",
            "value": "Downloading tokenizer_config.json: 100%"
          }
        },
        "4b786ede155a4bd393f04698b08cb8ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9b04ee6e02e43ec8f14eeacd8bf5e02",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a588fa60aee4eaa9512175424dbff04",
            "value": 28
          }
        },
        "8637da21baa342dbbe09a5b10e84cda3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13c533832e1147a99baa214211d1ea93",
            "placeholder": "​",
            "style": "IPY_MODEL_e8abcebcca784e17a6360c4bca4619f4",
            "value": " 28.0/28.0 [00:00&lt;00:00, 2.13kB/s]"
          }
        },
        "13f3b292cf1d4f44ac24f116814c01b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3a1ebf6a3eb4aa89368b6e46ae04bd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2dddb634372d42aaab1a6aa4207ba24a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9b04ee6e02e43ec8f14eeacd8bf5e02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a588fa60aee4eaa9512175424dbff04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "13c533832e1147a99baa214211d1ea93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8abcebcca784e17a6360c4bca4619f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "288c2513e3824b06bf2af1e6d79cd0e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a2bc226eb6b74d22bef9e61c3cb40b84",
              "IPY_MODEL_c4a9844ce16c47e9ba64bdc458d3d5e7",
              "IPY_MODEL_8fee7fe5486f48fc964f6b310c3000c6"
            ],
            "layout": "IPY_MODEL_3bdec310459d4e5fb6675f2bc8dd1f51"
          }
        },
        "a2bc226eb6b74d22bef9e61c3cb40b84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3703bcd981e54210b09b4ec878082ef2",
            "placeholder": "​",
            "style": "IPY_MODEL_a563da5f92ed43d1a4872dea126739ec",
            "value": "Downloading config.json: 100%"
          }
        },
        "c4a9844ce16c47e9ba64bdc458d3d5e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bdb7855e696441b92e3dbce7a24cef9",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7b3dd09494474f989c379df1f7fa3d3f",
            "value": 570
          }
        },
        "8fee7fe5486f48fc964f6b310c3000c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a605278f9057449f94a09728c2d0fcaa",
            "placeholder": "​",
            "style": "IPY_MODEL_4844d935f117498bbfd0f6eb69ce42eb",
            "value": " 570/570 [00:00&lt;00:00, 38.9kB/s]"
          }
        },
        "3bdec310459d4e5fb6675f2bc8dd1f51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3703bcd981e54210b09b4ec878082ef2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a563da5f92ed43d1a4872dea126739ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9bdb7855e696441b92e3dbce7a24cef9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b3dd09494474f989c379df1f7fa3d3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a605278f9057449f94a09728c2d0fcaa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4844d935f117498bbfd0f6eb69ce42eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Baiiiiiiiiii/ML2022-spring/blob/main/unmasked_teacher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## set up"
      ],
      "metadata": {
        "id": "6Q6vkpRIKrHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment"
      ],
      "metadata": {
        "id": "qmWIsjic5Z4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load data from google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wt0y7xDbKqlK",
        "outputId": "83dd7cd7-b872-40a7-986d-12947ff25a15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "gdrive\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MC-Ir5bdKGok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdb7a016-679b-4ac9-b389-107cd2200c43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/1lKO9qUQLnjUauiyqzl6VVs2RkwHidE3c/unmasked teacher\n",
            "clip_visual_encoder  __pycache__      uma_models  unmasked_teacher.ipynb\n",
            "configs\t\t     uma_cfg.py       uma_utils   unmasked_teacher_repo\n",
            "dataset\t\t     uma_checkpoints  umt_cfg.py\n"
          ]
        }
      ],
      "source": [
        "%cd ./gdrive/MyDrive/dlcv_final/unmasked_teacher\n",
        "!ls\n",
        "#!git clone https://github.com/OpenGVLab/unmasked_teacher.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install antlr4-python3-runtime==4.8\n",
        "!pip install attrs==21.4.0\n",
        "!pip install av==8.1.0\n",
        "!pip install backcall==0.2.0\n",
        "!pip install certifi==2021.10.8\n",
        "!pip install charset-normalizer==2.0.9\n",
        "!pip install click==8.0.3\n",
        "!pip install configparser==5.2.0\n",
        "!pip install decorator==5.1.0\n",
        "!pip install decord==0.6.0\n",
        "!pip install docker-pycreds==0.4.0\n",
        "!pip install easydict==1.9\n",
        "!pip install einops==0.4.1\n",
        "!pip install ffmpeg-python==0.2.0\n",
        "!pip install filelock==3.4.2\n",
        "!pip install ftfy==6.1.1\n",
        "!pip install future==0.18.2\n",
        "!pip install gitdb==4.0.9\n",
        "!pip install gitpython==3.1.24\n",
        "!pip install h5py==3.7.0\n",
        "!pip install huggingface-hub==0.2.1\n",
        "!pip install importlib-metadata==4.10.0\n",
        "!pip install iopath==0.1.10\n",
        "!pip install ipdb==0.13.9\n",
        "!pip install ipython==7.30.1\n",
        "!pip install jedi==0.18.1\n",
        "!pip install joblib==1.1.0\n",
        "!pip install matplotlib-inline==0.1.3\n",
        "!pip install omegaconf==2.1.1\n",
        "!pip install opencv-python==4.5.5.62\n",
        "!pip install packaging==21.3\n",
        "!pip install pandas==1.3.5\n",
        "!pip install paragraph==1.2.1\n",
        "!pip install parso==0.8.3\n",
        "!pip install pathtools==0.1.2\n",
        "!pip install pexpect==4.8.0\n",
        "!pip install pickleshare==0.7.5\n",
        "!pip install plotext==4.1.5\n",
        "!pip install portalocker==2.5.1\n",
        "!pip install promise==2.3\n",
        "!pip install prompt-toolkit==3.0.24\n",
        "!pip install protobuf==3.19.1\n",
        "!pip install psutil==5.9.0\n",
        "!pip install ptyprocess==0.7.0\n",
        "!pip install pygments==2.10.0\n",
        "!pip install pyparsing==3.0.6\n",
        "!pip install python-dateutil==2.8.2\n",
        "!pip install pytz==2021.3\n",
        "!pip install pyyaml==6.0\n",
        "!pip install regex==2021.11.10\n",
        "!pip install requests==2.26.0\n",
        "!pip install sacremoses==0.0.46\n",
        "!pip install scipy==1.7.3\n",
        "!pip install sentry-sdk==1.5.1\n",
        "!pip install shortuuid==1.0.8\n",
        "!pip install smmap==5.0.0\n",
        "!pip install subprocess32==3.5.4\n",
        "!pip install termcolor==1.1.0\n",
        "!pip install timm==0.4.12\n",
        "!pip install tokenizers==0.12.1\n",
        "!pip install toml==0.10.2\n",
        "!pip install tqdm==4.62.3\n",
        "!pip install traitlets==5.1.1\n",
        "!pip install transformers==4.21.2\n",
        "!pip install urllib3==1.26.7\n",
        "!pip install wandb==0.12.9\n",
        "!pip install wcwidth==0.2.5\n",
        "!pip install yaspin==2.1.0\n",
        "!pip install zipp==3.6.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h7FcSPvYNlAW",
        "outputId": "1240e5ba-3bc0-423d-8ba1-ede7534f041b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=40112a149ff8fabf9fb67acc5afa5b932409af80d44d161ef4e9fb6af1fc1a24\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime\n",
            "Successfully installed antlr4-python3-runtime-4.8\n",
            "Collecting attrs==21.4.0\n",
            "  Downloading attrs-21.4.0-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: attrs\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 23.1.0\n",
            "    Uninstalling attrs-23.1.0:\n",
            "      Successfully uninstalled attrs-23.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "jsonschema 4.19.2 requires attrs>=22.2.0, but you have attrs 21.4.0 which is incompatible.\n",
            "referencing 0.32.0 requires attrs>=22.2.0, but you have attrs 21.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed attrs-21.4.0\n",
            "Collecting av==8.1.0\n",
            "  Downloading av-8.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.6/36.6 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-8.1.0\n",
            "Requirement already satisfied: backcall==0.2.0 in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Collecting certifi==2021.10.8\n",
            "  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.2/149.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: certifi\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2023.11.17\n",
            "    Uninstalling certifi-2023.11.17:\n",
            "      Successfully uninstalled certifi-2023.11.17\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed certifi-2021.10.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting charset-normalizer==2.0.9\n",
            "  Downloading charset_normalizer-2.0.9-py3-none-any.whl (39 kB)\n",
            "Installing collected packages: charset-normalizer\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.3.2\n",
            "    Uninstalling charset-normalizer-3.3.2:\n",
            "      Successfully uninstalled charset-normalizer-3.3.2\n",
            "Successfully installed charset-normalizer-2.0.9\n",
            "Collecting click==8.0.3\n",
            "  Downloading click-8.0.3-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.5/97.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: click\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.7\n",
            "    Uninstalling click-8.1.7:\n",
            "      Successfully uninstalled click-8.1.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed click-8.0.3\n",
            "Collecting configparser==5.2.0\n",
            "  Downloading configparser-5.2.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: configparser\n",
            "Successfully installed configparser-5.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "configparser"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting decorator==5.1.0\n",
            "  Downloading decorator-5.1.0-py3-none-any.whl (9.1 kB)\n",
            "Installing collected packages: decorator\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 4.4.2\n",
            "    Uninstalling decorator-4.4.2:\n",
            "      Successfully uninstalled decorator-4.4.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed decorator-5.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "decorator"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting decord==0.6.0\n",
            "  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from decord==0.6.0) (1.23.5)\n",
            "Installing collected packages: decord\n",
            "Successfully installed decord-0.6.0\n",
            "Collecting docker-pycreds==0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds==0.4.0) (1.16.0)\n",
            "Installing collected packages: docker-pycreds\n",
            "Successfully installed docker-pycreds-0.4.0\n",
            "Collecting easydict==1.9\n",
            "  Downloading easydict-1.9.tar.gz (6.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: easydict\n",
            "  Building wheel for easydict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for easydict: filename=easydict-1.9-py3-none-any.whl size=6344 sha256=20481d3a3186aac48823564f4bde080490ab22ad49243e91938897ae90a511be\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/d2/35/4c11d19a72280492846f4c4df975311a2bac475e8021f86c1d\n",
            "Successfully built easydict\n",
            "Installing collected packages: easydict\n",
            "  Attempting uninstall: easydict\n",
            "    Found existing installation: easydict 1.11\n",
            "    Uninstalling easydict-1.11:\n",
            "      Successfully uninstalled easydict-1.11\n",
            "Successfully installed easydict-1.9\n",
            "Collecting einops==0.4.1\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.4.1\n",
            "Collecting ffmpeg-python==0.2.0\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python==0.2.0) (0.18.3)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n",
            "Collecting filelock==3.4.2\n",
            "  Downloading filelock-3.4.2-py3-none-any.whl (9.9 kB)\n",
            "Installing collected packages: filelock\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.13.1\n",
            "    Uninstalling filelock-3.13.1:\n",
            "      Successfully uninstalled filelock-3.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ibis-framework 6.2.0 requires filelock<4,>=3.7.0, but you have filelock 3.4.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filelock-3.4.2\n",
            "Collecting ftfy==6.1.1\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy==6.1.1) (0.2.12)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n",
            "Collecting future==0.18.2\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.2/829.2 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=cd7ddfe298e52379161b568a6d595c89a6738b7c0794c4a7432d237f29e825cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/73/06/557dc4f4ef68179b9d763930d6eec26b88ed7c389b19588a1c\n",
            "Successfully built future\n",
            "Installing collected packages: future\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.18.3\n",
            "    Uninstalling future-0.18.3:\n",
            "      Successfully uninstalled future-0.18.3\n",
            "Successfully installed future-0.18.2\n",
            "Collecting gitdb==4.0.9\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1 (from gitdb==4.0.9)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb\n",
            "Successfully installed gitdb-4.0.9 smmap-5.0.1\n",
            "Collecting gitpython==3.1.24\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.1/180.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython==3.1.24) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython==3.1.24) (5.0.1)\n",
            "Installing collected packages: gitpython\n",
            "Successfully installed gitpython-3.1.24\n",
            "Collecting h5py==3.7.0\n",
            "  Downloading h5py-3.7.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.10/dist-packages (from h5py==3.7.0) (1.23.5)\n",
            "Installing collected packages: h5py\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.9.0\n",
            "    Uninstalling h5py-3.9.0:\n",
            "      Successfully uninstalled h5py-3.9.0\n",
            "Successfully installed h5py-3.7.0\n",
            "Collecting huggingface-hub==0.2.1\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.2.1) (3.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.2.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.2.1) (4.66.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.2.1) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.2.1) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.2.1) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.2.1) (2.0.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.2.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.2.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.2.1) (2021.10.8)\n",
            "Installing collected packages: huggingface-hub\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.19.4\n",
            "    Uninstalling huggingface-hub-0.19.4:\n",
            "      Successfully uninstalled huggingface-hub-0.19.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tokenizers 0.15.0 requires huggingface_hub<1.0,>=0.16.4, but you have huggingface-hub 0.2.1 which is incompatible.\n",
            "transformers 4.35.2 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 0.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.2.1\n",
            "Collecting importlib-metadata==4.10.0\n",
            "  Downloading importlib_metadata-4.10.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata==4.10.0) (3.17.0)\n",
            "Installing collected packages: importlib-metadata\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 7.0.0\n",
            "    Uninstalling importlib-metadata-7.0.0:\n",
            "      Successfully uninstalled importlib-metadata-7.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dask 2023.8.1 requires importlib-metadata>=4.13.0, but you have importlib-metadata 4.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed importlib-metadata-4.10.0\n",
            "Collecting iopath==0.1.10\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from iopath==0.1.10) (4.66.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath==0.1.10) (4.5.0)\n",
            "Collecting portalocker (from iopath==0.1.10)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Building wheels for collected packages: iopath\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=09e7515be2af3be2fba3aac40d46bef71110a83b663a431b1cd55bc25e40b64e\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built iopath\n",
            "Installing collected packages: portalocker, iopath\n",
            "Successfully installed iopath-0.1.10 portalocker-2.8.2\n",
            "Collecting ipdb==0.13.9\n",
            "  Downloading ipdb-0.13.9.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from ipdb==0.13.9) (67.7.2)\n",
            "Requirement already satisfied: ipython>=7.17.0 in /usr/local/lib/python3.10/dist-packages (from ipdb==0.13.9) (7.34.0)\n",
            "Requirement already satisfied: toml>=0.10.2 in /usr/local/lib/python3.10/dist-packages (from ipdb==0.13.9) (0.10.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipdb==0.13.9) (5.1.0)\n",
            "Collecting jedi>=0.16 (from ipython>=7.17.0->ipdb==0.13.9)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7.17.0->ipdb==0.13.9) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.17.0->ipdb==0.13.9) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.17.0->ipdb==0.13.9) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=7.17.0->ipdb==0.13.9) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7.17.0->ipdb==0.13.9) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=7.17.0->ipdb==0.13.9) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.17.0->ipdb==0.13.9) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.17.0->ipdb==0.13.9) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=7.17.0->ipdb==0.13.9) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.17.0->ipdb==0.13.9) (0.2.12)\n",
            "Building wheels for collected packages: ipdb\n",
            "  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipdb: filename=ipdb-0.13.9-py3-none-any.whl size=11626 sha256=dd35deb2a9e3e76302bb73c04138680f3ee420fa86dda88379fcb0b96532d0e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/02/23/574616a4405110c3a9ccf58c81a7bdea1145360f54adb240a5\n",
            "Successfully built ipdb\n",
            "Installing collected packages: jedi, ipdb\n",
            "Successfully installed ipdb-0.13.9 jedi-0.19.1\n",
            "Collecting ipython==7.30.1\n",
            "  Downloading ipython-7.30.1-py3-none-any.whl (791 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m791.7/791.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython==7.30.1) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython==7.30.1) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython==7.30.1) (5.1.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython==7.30.1) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython==7.30.1) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython==7.30.1) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython==7.30.1) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython==7.30.1) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython==7.30.1) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython==7.30.1) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython==7.30.1) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython==7.30.1) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.30.1) (0.2.12)\n",
            "Installing collected packages: ipython\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 7.34.0\n",
            "    Uninstalling ipython-7.34.0:\n",
            "      Successfully uninstalled ipython-7.34.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipython==7.34.0, but you have ipython 7.30.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ipython-7.30.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jedi==0.18.1\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi==0.18.1) (0.8.3)\n",
            "Installing collected packages: jedi\n",
            "  Attempting uninstall: jedi\n",
            "    Found existing installation: jedi 0.19.1\n",
            "    Uninstalling jedi-0.19.1:\n",
            "      Successfully uninstalled jedi-0.19.1\n",
            "Successfully installed jedi-0.18.1\n",
            "Collecting joblib==1.1.0\n",
            "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.0/307.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: joblib\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.3.2\n",
            "    Uninstalling joblib-1.3.2:\n",
            "      Successfully uninstalled joblib-1.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imbalanced-learn 0.10.1 requires joblib>=1.1.1, but you have joblib 1.1.0 which is incompatible.\n",
            "scikit-learn 1.2.2 requires joblib>=1.1.1, but you have joblib 1.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed joblib-1.1.0\n",
            "Collecting matplotlib-inline==0.1.3\n",
            "  Downloading matplotlib_inline-0.1.3-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.10/dist-packages (from matplotlib-inline==0.1.3) (5.7.1)\n",
            "Installing collected packages: matplotlib-inline\n",
            "  Attempting uninstall: matplotlib-inline\n",
            "    Found existing installation: matplotlib-inline 0.1.6\n",
            "    Uninstalling matplotlib-inline-0.1.6:\n",
            "      Successfully uninstalled matplotlib-inline-0.1.6\n",
            "Successfully installed matplotlib-inline-0.1.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib_inline"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting omegaconf==2.1.1\n",
            "  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/74.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.7/74.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.10/dist-packages (from omegaconf==2.1.1) (4.8)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf==2.1.1) (6.0.1)\n",
            "Installing collected packages: omegaconf\n",
            "Successfully installed omegaconf-2.1.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencv-python==4.5.5.62\n",
            "  Downloading opencv_python-4.5.5.62-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python==4.5.5.62) (1.23.5)\n",
            "Installing collected packages: opencv-python\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.8.0.76\n",
            "    Uninstalling opencv-python-4.8.0.76:\n",
            "      Successfully uninstalled opencv-python-4.8.0.76\n",
            "Successfully installed opencv-python-4.5.5.62\n",
            "Collecting packaging==21.3\n",
            "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from packaging==21.3) (3.1.1)\n",
            "Installing collected packages: packaging\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 23.2\n",
            "    Uninstalling packaging-23.2:\n",
            "      Successfully uninstalled packaging-23.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "dask 2023.8.1 requires importlib-metadata>=4.13.0, but you have importlib-metadata 4.10.0 which is incompatible.\n",
            "tokenizers 0.15.0 requires huggingface_hub<1.0,>=0.16.4, but you have huggingface-hub 0.2.1 which is incompatible.\n",
            "transformers 4.35.2 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 0.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed packaging-21.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "packaging"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas==1.3.5\n",
            "  Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from pandas==1.3.5) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.10/dist-packages (from pandas==1.3.5) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==1.3.5) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7.3->pandas==1.3.5) (1.16.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "bigframes 0.17.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 1.3.5 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython==7.34.0, but you have ipython 7.30.1 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 1.3.5 which is incompatible.\n",
            "ibis-framework 6.2.0 requires filelock<4,>=3.7.0, but you have filelock 3.4.2 which is incompatible.\n",
            "plotnine 0.12.4 requires pandas>=1.5.0, but you have pandas 1.3.5 which is incompatible.\n",
            "xarray 2023.7.0 requires pandas>=1.4, but you have pandas 1.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-1.3.5\n",
            "Collecting paragraph==1.2.1\n",
            "  Downloading paragraph-1.2.1.tar.gz (29 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.10/dist-packages (from paragraph==1.2.1) (21.4.0)\n",
            "Building wheels for collected packages: paragraph\n",
            "  Building wheel for paragraph (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for paragraph: filename=paragraph-1.2.1-py3-none-any.whl size=12021 sha256=21ed7313447159660c3726bca3bd25cbbe4e2e83a10d53888d2ddbbd28cab200\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/d4/8e/947dcbeb8b3399b7a0ef60709131ba46dd8da682b045f71169\n",
            "Successfully built paragraph\n",
            "Installing collected packages: paragraph\n",
            "Successfully installed paragraph-1.2.1\n",
            "Requirement already satisfied: parso==0.8.3 in /usr/local/lib/python3.10/dist-packages (0.8.3)\n",
            "Collecting pathtools==0.1.2\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8793 sha256=6888ec675b631a5d146aa3141c8378d2e3e07fb3de2f917b8671142ba825189d\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools\n",
            "Successfully installed pathtools-0.1.2\n",
            "Collecting pexpect==4.8.0\n",
            "  Downloading pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.0/59.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect==4.8.0) (0.7.0)\n",
            "Installing collected packages: pexpect\n",
            "  Attempting uninstall: pexpect\n",
            "    Found existing installation: pexpect 4.9.0\n",
            "    Uninstalling pexpect-4.9.0:\n",
            "      Successfully uninstalled pexpect-4.9.0\n",
            "Successfully installed pexpect-4.8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pexpect"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.10/dist-packages (0.7.5)\n",
            "Collecting plotext==4.1.5\n",
            "  Downloading plotext-4.1.5-py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: plotext\n",
            "Successfully installed plotext-4.1.5\n",
            "Collecting portalocker==2.5.1\n",
            "  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: portalocker\n",
            "  Attempting uninstall: portalocker\n",
            "    Found existing installation: portalocker 2.8.2\n",
            "    Uninstalling portalocker-2.8.2:\n",
            "      Successfully uninstalled portalocker-2.8.2\n",
            "Successfully installed portalocker-2.5.1\n",
            "Requirement already satisfied: promise==2.3 in /usr/local/lib/python3.10/dist-packages (2.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from promise==2.3) (1.16.0)\n",
            "Collecting prompt-toolkit==3.0.24\n",
            "  Downloading prompt_toolkit-3.0.24-py3-none-any.whl (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.7/374.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit==3.0.24) (0.2.12)\n",
            "Installing collected packages: prompt-toolkit\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 3.0.43\n",
            "    Uninstalling prompt-toolkit-3.0.43:\n",
            "      Successfully uninstalled prompt-toolkit-3.0.43\n",
            "Successfully installed prompt-toolkit-3.0.24\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting protobuf==3.19.1\n",
            "  Downloading protobuf-3.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-ai-generativelanguage 0.4.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.1 which is incompatible.\n",
            "google-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.19.1 which is incompatible.\n",
            "google-cloud-aiplatform 1.38.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.1 which is incompatible.\n",
            "google-cloud-bigquery 3.12.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.1 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.1 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.24.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.1 which is incompatible.\n",
            "google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.1 which is incompatible.\n",
            "google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.1 which is incompatible.\n",
            "google-cloud-functions 1.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.1 which is incompatible.\n",
            "google-cloud-iam 2.13.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.1 which is incompatible.\n",
            "google-cloud-language 2.9.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.1 which is incompatible.\n",
            "google-cloud-resource-manager 1.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.1 which is incompatible.\n",
            "google-cloud-translate 3.11.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.1 which is incompatible.\n",
            "googleapis-common-protos 1.62.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.19.1 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.1 which is incompatible.\n",
            "tensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 3.19.1 which is incompatible.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.1 which is incompatible.\n",
            "tensorflow-datasets 4.9.4 requires protobuf>=3.20, but you have protobuf 3.19.1 which is incompatible.\n",
            "tensorflow-hub 0.15.0 requires protobuf>=3.19.6, but you have protobuf 3.19.1 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.19.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.19.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting psutil==5.9.0\n",
            "  Downloading psutil-5.9.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/281.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/281.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: psutil\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.5\n",
            "    Uninstalling psutil-5.9.5:\n",
            "      Successfully uninstalled psutil-5.9.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-datasets 4.9.4 requires protobuf>=3.20, but you have protobuf 3.19.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed psutil-5.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ptyprocess==0.7.0 in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Collecting pygments==2.10.0\n",
            "  Downloading Pygments-2.10.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pygments\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.16.1\n",
            "    Uninstalling Pygments-2.16.1:\n",
            "      Successfully uninstalled Pygments-2.16.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "rich 13.7.0 requires pygments<3.0.0,>=2.13.0, but you have pygments 2.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pygments-2.10.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pygments"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyparsing==3.0.6\n",
            "  Downloading pyparsing-3.0.6-py3-none-any.whl (97 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/97.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m92.2/97.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyparsing\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.1.1\n",
            "    Uninstalling pyparsing-3.1.1:\n",
            "      Successfully uninstalled pyparsing-3.1.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "plotnine 0.12.4 requires pandas>=1.5.0, but you have pandas 1.3.5 which is incompatible.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.1 which is incompatible.\n",
            "transformers 4.35.2 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 0.2.1 which is incompatible.\n",
            "xarray 2023.7.0 requires pandas>=1.4, but you have pandas 1.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pyparsing-3.0.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyparsing"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.10/dist-packages (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil==2.8.2) (1.16.0)\n",
            "Collecting pytz==2021.3\n",
            "  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.5/503.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytz\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2023.3.post1\n",
            "    Uninstalling pytz-2023.3.post1:\n",
            "      Successfully uninstalled pytz-2023.3.post1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ibis-framework 6.2.0 requires filelock<4,>=3.7.0, but you have filelock 3.4.2 which is incompatible.\n",
            "ibis-framework 6.2.0 requires pytz>=2022.7, but you have pytz 2021.3 which is incompatible.\n",
            "plotnine 0.12.4 requires pandas>=1.5.0, but you have pandas 1.3.5 which is incompatible.\n",
            "xarray 2023.7.0 requires pandas>=1.4, but you have pandas 1.3.5 which is incompatible.\n",
            "yfinance 0.2.33 requires pytz>=2022.5, but you have pytz 2021.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pytz-2021.3\n",
            "Collecting pyyaml==6.0\n",
            "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyyaml\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0.1\n",
            "    Uninstalling PyYAML-6.0.1:\n",
            "      Successfully uninstalled PyYAML-6.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "dask 2023.8.1 requires importlib-metadata>=4.13.0, but you have importlib-metadata 4.10.0 which is incompatible.\n",
            "tokenizers 0.15.0 requires huggingface_hub<1.0,>=0.16.4, but you have huggingface-hub 0.2.1 which is incompatible.\n",
            "transformers 4.35.2 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 0.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pyyaml-6.0\n",
            "Collecting regex==2021.11.10\n",
            "  Downloading regex-2021.11.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.0/764.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: regex\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2023.6.3\n",
            "    Uninstalling regex-2023.6.3:\n",
            "      Successfully uninstalled regex-2023.6.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.35.2 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 0.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed regex-2021.11.10\n",
            "Collecting requests==2.26.0\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.21.1 (from requests==2.26.0)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.26.0) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests==2.26.0) (2.0.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.26.0) (3.6)\n",
            "Installing collected packages: urllib3, requests\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 0.17.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 1.3.5 which is incompatible.\n",
            "bigframes 0.17.0 requires requests>=2.27.1, but you have requests 2.26.0 which is incompatible.\n",
            "google-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.19.1 which is incompatible.\n",
            "google-cloud-aiplatform 1.38.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.1 which is incompatible.\n",
            "google-cloud-bigquery 3.12.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.1 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.1 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.24.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.1 which is incompatible.\n",
            "google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.1 which is incompatible.\n",
            "google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.1 which is incompatible.\n",
            "google-cloud-functions 1.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.1 which is incompatible.\n",
            "google-cloud-iam 2.13.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.1 which is incompatible.\n",
            "google-cloud-language 2.9.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.1 which is incompatible.\n",
            "google-cloud-resource-manager 1.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.1 which is incompatible.\n",
            "google-cloud-translate 3.11.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.1 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython==7.34.0, but you have ipython 7.30.1 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 1.3.5 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.26.0 which is incompatible.\n",
            "moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.0 which is incompatible.\n",
            "tensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 3.19.1 which is incompatible.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.1 which is incompatible.\n",
            "tensorflow-datasets 4.9.4 requires protobuf>=3.20, but you have protobuf 3.19.1 which is incompatible.\n",
            "tokenizers 0.15.0 requires huggingface_hub<1.0,>=0.16.4, but you have huggingface-hub 0.2.1 which is incompatible.\n",
            "transformers 4.35.2 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 0.2.1 which is incompatible.\n",
            "tweepy 4.14.0 requires requests<3,>=2.27.0, but you have requests 2.26.0 which is incompatible.\n",
            "yfinance 0.2.33 requires pytz>=2022.5, but you have pytz 2021.3 which is incompatible.\n",
            "yfinance 0.2.33 requires requests>=2.31, but you have requests 2.26.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed requests-2.26.0 urllib3-1.26.18\n",
            "Collecting sacremoses==0.0.46\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m895.2/895.2 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.0.46) (2021.11.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.0.46) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.0.46) (8.0.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.0.46) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.0.46) (4.66.1)\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.0.46\n",
            "Collecting scipy==1.7.3\n",
            "  Downloading scipy-1.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy<1.23.0,>=1.16.5 (from scipy==1.7.3)\n",
            "  Downloading numpy-1.22.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.4\n",
            "    Uninstalling scipy-1.11.4:\n",
            "      Successfully uninstalled scipy-1.11.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "arviz 0.15.1 requires scipy>=1.8.0, but you have scipy 1.7.3 which is incompatible.\n",
            "ibis-framework 6.2.0 requires filelock<4,>=3.7.0, but you have filelock 3.4.2 which is incompatible.\n",
            "ibis-framework 6.2.0 requires pytz>=2022.7, but you have pytz 2021.3 which is incompatible.\n",
            "imbalanced-learn 0.10.1 requires joblib>=1.1.1, but you have joblib 1.1.0 which is incompatible.\n",
            "jax 0.4.23 requires scipy>=1.9, but you have scipy 1.7.3 which is incompatible.\n",
            "jaxlib 0.4.23+cuda12.cudnn89 requires scipy>=1.9, but you have scipy 1.7.3 which is incompatible.\n",
            "moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.0 which is incompatible.\n",
            "plotnine 0.12.4 requires numpy>=1.23.0, but you have numpy 1.22.4 which is incompatible.\n",
            "plotnine 0.12.4 requires pandas>=1.5.0, but you have pandas 1.3.5 which is incompatible.\n",
            "scikit-learn 1.2.2 requires joblib>=1.1.1, but you have joblib 1.1.0 which is incompatible.\n",
            "tensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 3.19.1 which is incompatible.\n",
            "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.22.4 which is incompatible.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.1 which is incompatible.\n",
            "tensorflow-datasets 4.9.4 requires protobuf>=3.20, but you have protobuf 3.19.1 which is incompatible.\n",
            "tensorflow-hub 0.15.0 requires protobuf>=3.19.6, but you have protobuf 3.19.1 which is incompatible.\n",
            "transformers 4.35.2 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 0.2.1 which is incompatible.\n",
            "xarray 2023.7.0 requires pandas>=1.4, but you have pandas 1.3.5 which is incompatible.\n",
            "yfinance 0.2.33 requires pytz>=2022.5, but you have pytz 2021.3 which is incompatible.\n",
            "yfinance 0.2.33 requires requests>=2.31, but you have requests 2.26.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.22.4 scipy-1.7.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentry-sdk==1.5.1\n",
            "  Downloading sentry_sdk-1.5.1-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.9/140.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from sentry-sdk==1.5.1) (1.26.18)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from sentry-sdk==1.5.1) (2021.10.8)\n",
            "Installing collected packages: sentry-sdk\n",
            "Successfully installed sentry-sdk-1.5.1\n",
            "Collecting shortuuid==1.0.8\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Installing collected packages: shortuuid\n",
            "Successfully installed shortuuid-1.0.8\n",
            "Collecting smmap==5.0.0\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap\n",
            "  Attempting uninstall: smmap\n",
            "    Found existing installation: smmap 5.0.1\n",
            "    Uninstalling smmap-5.0.1:\n",
            "      Successfully uninstalled smmap-5.0.1\n",
            "Successfully installed smmap-5.0.0\n",
            "Collecting subprocess32==3.5.4\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.4/97.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: subprocess32\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6487 sha256=70ad6bd207de6066907e1a2cb4bfa7ffdd8f5fa7424b9cf1db832cb487cfbc0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/19/61/d440ccd46a2a014bce61fc5c6c8495dedd32ef04cba8b34b28\n",
            "Successfully built subprocess32\n",
            "Installing collected packages: subprocess32\n",
            "Successfully installed subprocess32-3.5.4\n",
            "Collecting termcolor==1.1.0\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: termcolor\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=5d6005fa0f61a39aadfe75144b2fd9ec60b2ef8a2dada7d9b1997f78f54ac080\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/49/46/1b13a65d8da11238af9616b00fdde6d45b0f95d9291bac8452\n",
            "Successfully built termcolor\n",
            "Installing collected packages: termcolor\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.4.0\n",
            "    Uninstalling termcolor-2.4.0:\n",
            "      Successfully uninstalled termcolor-2.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.22.4 which is incompatible.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.1 which is incompatible.\n",
            "tensorflow-datasets 4.9.4 requires protobuf>=3.20, but you have protobuf 3.19.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed termcolor-1.1.0\n",
            "Collecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.10/dist-packages (from timm==0.4.12) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm==0.4.12) (0.16.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (3.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.4.12) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.4.12) (2.26.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.4.12) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4->timm==0.4.12) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->timm==0.4.12) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->timm==0.4.12) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->timm==0.4.12) (2.0.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->timm==0.4.12) (3.6)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4->timm==0.4.12) (1.3.0)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.4.12\n",
            "Collecting tokenizers==0.12.1\n",
            "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.0\n",
            "    Uninstalling tokenizers-0.15.0:\n",
            "      Successfully uninstalled tokenizers-0.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.35.2 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 0.2.1 which is incompatible.\n",
            "transformers 4.35.2 requires tokenizers<0.19,>=0.14, but you have tokenizers 0.12.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.12.1\n",
            "Requirement already satisfied: toml==0.10.2 in /usr/local/lib/python3.10/dist-packages (0.10.2)\n",
            "Collecting tqdm==4.62.3\n",
            "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tqdm\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.1\n",
            "    Uninstalling tqdm-4.66.1:\n",
            "      Successfully uninstalled tqdm-4.66.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.0 which is incompatible.\n",
            "tensorflow-datasets 4.9.4 requires protobuf>=3.20, but you have protobuf 3.19.1 which is incompatible.\n",
            "transformers 4.35.2 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 0.2.1 which is incompatible.\n",
            "transformers 4.35.2 requires tokenizers<0.19,>=0.14, but you have tokenizers 0.12.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tqdm-4.62.3\n",
            "Collecting traitlets==5.1.1\n",
            "  Downloading traitlets-5.1.1-py3-none-any.whl (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.0/102.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: traitlets\n",
            "  Attempting uninstall: traitlets\n",
            "    Found existing installation: traitlets 5.7.1\n",
            "    Uninstalling traitlets-5.7.1:\n",
            "      Successfully uninstalled traitlets-5.7.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-core 5.5.1 requires traitlets>=5.3, but you have traitlets 5.1.1 which is incompatible.\n",
            "nbclient 0.9.0 requires traitlets>=5.4, but you have traitlets 5.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed traitlets-5.1.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "traitlets"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.21.2\n",
            "  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.21.2) (3.4.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.21.2) (0.2.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.21.2) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.21.2) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.21.2) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.21.2) (2021.11.10)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.21.2) (2.26.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.21.2) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.21.2) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.21.2) (4.5.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from packaging>=20.0->transformers==4.21.2) (3.0.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.21.2) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.21.2) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.21.2) (2.0.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.21.2) (3.6)\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed transformers-4.21.2\n",
            "Collecting urllib3==1.26.7\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.8/138.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.18\n",
            "    Uninstalling urllib3-1.26.18:\n",
            "      Successfully uninstalled urllib3-1.26.18\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 3.19.1 which is incompatible.\n",
            "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.22.4 which is incompatible.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.1 which is incompatible.\n",
            "tensorflow-datasets 4.9.4 requires protobuf>=3.20, but you have protobuf 3.19.1 which is incompatible.\n",
            "tweepy 4.14.0 requires requests<3,>=2.27.0, but you have requests 2.26.0 which is incompatible.\n",
            "yfinance 0.2.33 requires pytz>=2022.5, but you have pytz 2021.3 which is incompatible.\n",
            "yfinance 0.2.33 requires requests>=2.31, but you have requests 2.26.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed urllib3-1.26.7\n",
            "Collecting wandb==0.12.9\n",
            "  Downloading wandb-0.12.9-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.12.9) (8.0.3)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.12.9) (3.1.24)\n",
            "Collecting yaspin>=1.0.0 (from wandb==0.12.9)\n",
            "  Downloading yaspin-3.0.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.10/dist-packages (from wandb==0.12.9) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.12.9) (2.26.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.12.9) (2.3)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.12.9) (1.0.8)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.12.9) (1.16.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.12.9) (5.9.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.12.9) (1.5.1)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.10/dist-packages (from wandb==0.12.9) (3.5.4)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.12.9) (0.4.0)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from wandb==0.12.9) (5.2.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.12.9) (3.19.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb==0.12.9) (6.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb==0.12.9) (0.1.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython>=1.0.0->wandb==0.12.9) (4.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb==0.12.9) (1.26.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb==0.12.9) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb==0.12.9) (2.0.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb==0.12.9) (3.6)\n",
            "Collecting termcolor<3.0,>=2.3 (from yaspin>=1.0.0->wandb==0.12.9)\n",
            "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb==0.12.9) (5.0.0)\n",
            "Installing collected packages: termcolor, yaspin, wandb\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 1.1.0\n",
            "    Uninstalling termcolor-1.1.0:\n",
            "      Successfully uninstalled termcolor-1.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.22.4 which is incompatible.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.1 which is incompatible.\n",
            "tensorflow-datasets 4.9.4 requires protobuf>=3.20, but you have protobuf 3.19.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed termcolor-2.4.0 wandb-0.12.9 yaspin-3.0.1\n",
            "Collecting wcwidth==0.2.5\n",
            "  Downloading wcwidth-0.2.5-py2.py3-none-any.whl (30 kB)\n",
            "Installing collected packages: wcwidth\n",
            "  Attempting uninstall: wcwidth\n",
            "    Found existing installation: wcwidth 0.2.12\n",
            "    Uninstalling wcwidth-0.2.12:\n",
            "      Successfully uninstalled wcwidth-0.2.12\n",
            "Successfully installed wcwidth-0.2.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "wcwidth"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yaspin==2.1.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Collecting termcolor<2.0.0,>=1.1.0 (from yaspin==2.1.0)\n",
            "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
            "Installing collected packages: termcolor, yaspin\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.4.0\n",
            "    Uninstalling termcolor-2.4.0:\n",
            "      Successfully uninstalled termcolor-2.4.0\n",
            "  Attempting uninstall: yaspin\n",
            "    Found existing installation: yaspin 3.0.1\n",
            "    Uninstalling yaspin-3.0.1:\n",
            "      Successfully uninstalled yaspin-3.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.22.4 which is incompatible.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.1 which is incompatible.\n",
            "tensorflow-datasets 4.9.4 requires protobuf>=3.20, but you have protobuf 3.19.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed termcolor-1.1.0 yaspin-2.1.0\n",
            "Collecting zipp==3.6.0\n",
            "  Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
            "Installing collected packages: zipp\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.17.0\n",
            "    Uninstalling zipp-3.17.0:\n",
            "      Successfully uninstalled zipp-3.17.0\n",
            "Successfully installed zipp-3.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.22.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "id": "5-Nv4sLe_bOy",
        "outputId": "347c505d-0736-48de-a92e-ef89faa9e5e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.22.0\n",
            "  Downloading transformers-4.22.0-py3-none-any.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.22.0) (3.4.2)\n",
            "Collecting huggingface-hub<1.0,>=0.9.0 (from transformers==4.22.0)\n",
            "  Downloading huggingface_hub-0.20.1-py3-none-any.whl (330 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.22.0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.22.0) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.22.0) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.22.0) (2021.11.10)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.22.0) (2.26.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.22.0) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.22.0) (4.62.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers==4.22.0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers==4.22.0) (4.5.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from packaging>=20.0->transformers==4.22.0) (3.0.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.22.0) (1.26.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.22.0) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.22.0) (2.0.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.22.0) (3.6)\n",
            "Installing collected packages: huggingface-hub, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.2.1\n",
            "    Uninstalling huggingface-hub-0.2.1:\n",
            "      Successfully uninstalled huggingface-hub-0.2.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.21.2\n",
            "    Uninstalling transformers-4.21.2:\n",
            "      Successfully uninstalled transformers-4.21.2\n",
            "Successfully installed huggingface-hub-0.20.1 transformers-4.22.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "huggingface_hub",
                  "transformers"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameters"
      ],
      "metadata": {
        "id": "LI9Fzvja5eEN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PSYU51miNBZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "VAmYst5ePlua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def save_json(content, save_path):\n",
        "    with open(save_path, 'w') as f:\n",
        "        f.write(json.dumps(content))\n",
        "\n",
        "# STAR\n",
        "# from https://github.com/Yui010206/SeViLA/blob/main/sevila_data/Data%20Preprocess.ipynb\n",
        "# preprocess metadata json files\n",
        "# train_path = '../dataset/STAR_train.json'\n",
        "# val_path = '../dataset/STAR_val.json'\n",
        "# train = json.load(open(train_path))\n",
        "# val = json.load(open(val_path))\n",
        "# new_train = []\n",
        "# new_val = []\n",
        "# for qa in train:\n",
        "#     qa_dict = {}\n",
        "#     qa_dict['video'] = qa['video_id']\n",
        "#     qa_dict['num_option'] = 4\n",
        "#     qa_dict['qid'] = qa['question_id']\n",
        "#     for i, choice in enumerate(qa['choices']):\n",
        "#         qa_dict['a{}'.format(str(i))] = choice['choice']\n",
        "#         if choice['choice'] == qa['answer']:\n",
        "#             answer = i\n",
        "#     qa_dict['answer'] = answer\n",
        "#     qa_dict['question'] = qa['question']\n",
        "#     qa_dict['start'] = qa['start']\n",
        "#     qa_dict['end'] = qa['end']\n",
        "#     new_train.append(qa_dict)\n",
        "\n",
        "# for qa in val:\n",
        "#     qa_dict = {}\n",
        "#     qa_dict['video'] = qa['video_id']\n",
        "#     qa_dict['num_option'] = 4\n",
        "#     qa_dict['qid'] = qa['question_id']\n",
        "#     for i, choice in enumerate(qa['choices']):\n",
        "#         qa_dict['a{}'.format(str(i))] = choice['choice']\n",
        "#         if choice['choice'] == qa['answer']:\n",
        "#             answer = i\n",
        "#     qa_dict['answer'] = answer\n",
        "#     qa_dict['question'] = qa['question']\n",
        "#     qa_dict['start'] = qa['start']\n",
        "#     qa_dict['end'] = qa['end']\n",
        "#     new_val.append(qa_dict)\n",
        "\n",
        "# save_json(new_train, 'star/train.json')\n",
        "# save_json(new_val, 'star/val.json')\n",
        "\n",
        "\n",
        "# After preprocess\n",
        "# [{\"video\": \"6H78U\", \"num_option\": 4, \"qid\": \"Interaction_T1_13\", \"a0\": \"The closet/cabinet.\", \"a1\": \"The blanket.\", \"a2\": \"The clothes.\", \"a3\": \"The table.\", \"answer\": 2, \"question\": \"Which object was tidied up by the person?\", \"start\": 11.1, \"end\": 19.6},\n",
        "#  {\"video\": \"6H78U\", \"num_option\": 4, \"qid\": \"Interaction_T1_14\", \"a0\": \"The blanket.\", \"a1\": \"The table.\", \"a2\": \"The clothes.\", \"a3\": \"The closet/cabinet.\", \"answer\": 2, \"question\": \"Which object was tidied up by the person?\", \"start\": 15.6, \"end\": 22.7}]\n",
        "\n",
        "\n",
        "import json\n",
        "import os\n",
        "from torchvision.io import read_video\n",
        "from torchvision.transforms import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class VideoTextDataset(Dataset):\n",
        "    def __init__(self, json_file='./dataset/STAR_train.json', video_folder='./dataset/Charades_v1_480'):\n",
        "        self.data = self.load_data(json_file)\n",
        "        self.video_folder = video_folder\n",
        "\n",
        "        self.parse_data = []\n",
        "        for qa in self.data:\n",
        "            qa_dict = {}\n",
        "            qa_dict['video_name'] = qa['video_id']\n",
        "            qa_dict['num_option'] = 4\n",
        "            qa_dict['qid'] = qa['question_id']\n",
        "            for i, choice in enumerate(qa['choices']):\n",
        "                qa_dict['a{}'.format(str(i))] = choice['choice']\n",
        "                # if choice['choice'] == qa['answer']:\n",
        "                #     answer = i\n",
        "            qa_dict['answer'] = qa['answer']\n",
        "            qa_dict['question'] = qa['question']\n",
        "            qa_dict['start'] = qa['start']\n",
        "            qa_dict['end'] = qa['end']\n",
        "            self.parse_data.append(qa_dict)\n",
        "\n",
        "\n",
        "    def load_data(self, json_file):\n",
        "        with open(json_file, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.parse_data[idx]\n",
        "\n",
        "        # Read video\n",
        "        video_path = os.path.join(self.video_folder, item['video_name'] + '.mp4')\n",
        "        video, audio, info = read_video(video_path, item[\"start\"], item[\"end\"], pts_unit='sec')\n",
        "        # video = read_video(video_path, item[\"start\"], item[\"end\"], pts_unit='sec')\n",
        "\n",
        "\n",
        "        # Process text (you might want to use a tokenizer here)\n",
        "        question_text = item['question']\n",
        "        answer_text = item['answer']\n",
        "        choices_text = [ item['a{}'.format(str(i))] for i in range(item[\"num_option\"])]\n",
        "\n",
        "        if question_text[-1] != '?':\n",
        "            question_text = str(question_text) + \"?\"\n",
        "\n",
        "        # concatenate question and choices text\n",
        "        gt_QApair = question_text + ' ' + 'answer: ' + answer_text\n",
        "        candidates_QApair = []\n",
        "        for i in range(item[\"num_option\"]):\n",
        "            candidates_QApair.append(question_text + ' ' + 'answer: ' + choices_text[i])\n",
        "\n",
        "\n",
        "        return video, candidates_QApair, gt_QApair, answer_text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     json_file = \"./dataset/STAR_train.json\"\n",
        "#     video_folder = \"./dataset/Charades_v1_480\"\n",
        "#     dataset = VideoTextDataset(json_file, video_folder)\n",
        "#     video, candidates_QApair, gt_QApair, ans = dataset[0]\n",
        "#     print(video.shape)\n",
        "#     print(candidates_QApair)\n",
        "#     print(gt_QApair)"
      ],
      "metadata": {
        "id": "qTGV7gtkPo6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "import random\n",
        "\n",
        "def save_json(content, save_path):\n",
        "    with open(save_path, 'w') as f:\n",
        "        f.write(json.dumps(content))\n",
        "\n",
        "\n",
        "# STAR\n",
        "# from https://github.com/Yui010206/SeViLA/blob/main/sevila_data/Data%20Preprocess.ipynb\n",
        "# preprocess metadata json files\n",
        "# train_path = '../dataset/STAR_train.json'\n",
        "# val_path = '../dataset/STAR_val.json'\n",
        "# train = json.load(open(train_path))\n",
        "# val = json.load(open(val_path))\n",
        "# new_train = []\n",
        "# new_val = []\n",
        "# for qa in train:\n",
        "#     qa_dict = {}\n",
        "#     qa_dict['video'] = qa['video_id']\n",
        "#     qa_dict['num_option'] = 4\n",
        "#     qa_dict['qid'] = qa['question_id']\n",
        "#     for i, choice in enumerate(qa['choices']):\n",
        "#         qa_dict['a{}'.format(str(i))] = choice['choice']\n",
        "#         if choice['choice'] == qa['answer']:\n",
        "#             answer = i\n",
        "#     qa_dict['answer'] = answer\n",
        "#     qa_dict['question'] = qa['question']\n",
        "#     qa_dict['start'] = qa['start']\n",
        "#     qa_dict['end'] = qa['end']\n",
        "#     new_train.append(qa_dict)\n",
        "\n",
        "# for qa in val:\n",
        "#     qa_dict = {}\n",
        "#     qa_dict['video'] = qa['video_id']\n",
        "#     qa_dict['num_option'] = 4\n",
        "#     qa_dict['qid'] = qa['question_id']\n",
        "#     for i, choice in enumerate(qa['choices']):\n",
        "#         qa_dict['a{}'.format(str(i))] = choice['choice']\n",
        "#         if choice['choice'] == qa['answer']:\n",
        "#             answer = i\n",
        "#     qa_dict['answer'] = answer\n",
        "#     qa_dict['question'] = qa['question']\n",
        "#     qa_dict['start'] = qa['start']\n",
        "#     qa_dict['end'] = qa['end']\n",
        "#     new_val.append(qa_dict)\n",
        "\n",
        "# save_json(new_train, 'star/train.json')\n",
        "# save_json(new_val, 'star/val.json')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# After preprocess\n",
        "# [{\"video\": \"6H78U\", \"num_option\": 4, \"qid\": \"Interaction_T1_13\", \"a0\": \"The closet/cabinet.\", \"a1\": \"The blanket.\", \"a2\": \"The clothes.\", \"a3\": \"The table.\", \"answer\": 2, \"question\": \"Which object was tidied up by the person?\", \"start\": 11.1, \"end\": 19.6},\n",
        "#  {\"video\": \"6H78U\", \"num_option\": 4, \"qid\": \"Interaction_T1_14\", \"a0\": \"The blanket.\", \"a1\": \"The table.\", \"a2\": \"The clothes.\", \"a3\": \"The closet/cabinet.\", \"answer\": 2, \"question\": \"Which object was tidied up by the person?\", \"start\": 15.6, \"end\": 22.7}]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "import os\n",
        "from torchvision.io import read_video\n",
        "from torchvision.transforms import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class VideoTextDataset(Dataset):\n",
        "    def __init__(self, json_file, video_folder, num_frame=8):\n",
        "        self.data = self.load_data(json_file)\n",
        "        self.video_folder = video_folder\n",
        "        self.num_frame = num_frame\n",
        "\n",
        "        self.parse_data = []\n",
        "        for qa in self.data:\n",
        "            qa_dict = {}\n",
        "            qa_dict['video_name'] = qa['video_id']\n",
        "            qa_dict['num_option'] = 4\n",
        "            qa_dict['qid'] = qa['question_id']\n",
        "            for i, choice in enumerate(qa['choices']):\n",
        "                qa_dict['a{}'.format(str(i))] = choice['choice']\n",
        "                # if choice['choice'] == qa['answer']:\n",
        "                #     answer = i\n",
        "            qa_dict['answer'] = qa['answer']\n",
        "            qa_dict['question'] = qa['question']\n",
        "            qa_dict['start'] = qa['start']\n",
        "            qa_dict['end'] = qa['end']\n",
        "            self.parse_data.append(qa_dict)\n",
        "\n",
        "\n",
        "    def load_data(self, json_file):\n",
        "        with open(json_file, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.parse_data[idx]\n",
        "        qid = item[\"qid\"]\n",
        "        # Read video\n",
        "        video_path = os.path.join(self.video_folder, item['video_name'] + '.mp4')\n",
        "        video, audio, info = read_video(video_path, item[\"start\"], item[\"end\"], pts_unit='sec')\n",
        "        video_fps = info[\"video_fps\"]\n",
        "\n",
        "        # Sample <num_frame=8> frames evenly from the video\n",
        "        total_frames = video.shape[0]\n",
        "        sampled_frames = torch.linspace(0, total_frames - 1, self.num_frame, dtype=torch.int)\n",
        "        video = video[sampled_frames]\n",
        "\n",
        "        # to advoid dimension error\n",
        "        if video.size()[1] == 270: video = video.permute(0, 2, 1, 3)\n",
        "        print(f\"video {idx} dim: {video.size()}\")\n",
        "\n",
        "        # Process text (you might want to use a tokenizer here)\n",
        "        question_text = item['question']\n",
        "        answer_text = item['answer']\n",
        "        choices_text = [ item['a{}'.format(str(i))] for i in range(item[\"num_option\"])]\n",
        "\n",
        "        if question_text[-1] != '?':\n",
        "            question_text = str(question_text) + \"?\"\n",
        "\n",
        "        # concatenate question and choices text\n",
        "        gt_QApair = question_text + ' ' + 'answer: ' + answer_text\n",
        "        candidates_QApair = []\n",
        "        for i in range(item[\"num_option\"]):\n",
        "            candidates_QApair.append(question_text + ' ' + 'answer: ' + choices_text[i])\n",
        "\n",
        "\n",
        "        return video, candidates_QApair, gt_QApair, qid, sampled_frames, answer_text\n",
        "\n",
        "\n",
        "'''\n",
        "if __name__ == \"__main__\":\n",
        "    json_file = \"/home/eric/temp/SeViLA/dataset/STAR_train.json\"\n",
        "    video_folder = \"/home/eric/temp/SeViLA/dataset/Charades_v1_480\"\n",
        "    dataset = VideoTextDataset(json_file, video_folder)\n",
        "    video, candidates_QApair, gt_QApair, qid, sampled_frames = dataset[0]\n",
        "    print(video.shape)\n",
        "    print(candidates_QApair)\n",
        "    print(gt_QApair)\n",
        "    print(qid)\n",
        "    # print(info)\n",
        "    print(sampled_frames)\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "PpkZ4zKLWD6F",
        "outputId": "ae786cc2-f352-4f48-95cd-321beaae938f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (5.2.0)/charset_normalizer (2.0.9) doesn't match a supported version!\n",
            "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nif __name__ == \"__main__\":\\n    json_file = \"/home/eric/temp/SeViLA/dataset/STAR_train.json\"\\n    video_folder = \"/home/eric/temp/SeViLA/dataset/Charades_v1_480\"\\n    dataset = VideoTextDataset(json_file, video_folder)\\n    video, candidates_QApair, gt_QApair, qid, sampled_frames = dataset[0]\\n    print(video.shape)\\n    print(candidates_QApair)\\n    print(gt_QApair)\\n    print(qid)\\n    # print(info)\\n    print(sampled_frames)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_file = \"./dataset/STAR_train.json\"\n",
        "video_folder = \"./dataset/Charades_v1_480\"\n",
        "dataset = VideoTextDataset(json_file, video_folder)"
      ],
      "metadata": {
        "id": "IjA0-Kn3rMRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  video, candidates_QApair, gt_QApair, qid, sampled_frames, answer_text = dataset[i]\n",
        "#print(video.shape)\n",
        "'''\n",
        "print(candidates_QApair)\n",
        "print(gt_QApair)\n",
        "print(qid)\n",
        "# print(info)\n",
        "print(sampled_frames)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un3C82gFrjFn",
        "outputId": "5ebc22bf-96e3-4a91-d037-43d10756de1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "video 0 dim: torch.Size([8, 480, 270, 3])\n",
            "video 1 dim: torch.Size([8, 480, 270, 3])\n",
            "video 2 dim: torch.Size([8, 480, 270, 3])\n",
            "video 3 dim: torch.Size([8, 480, 270, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "f4R5Pscem9eF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load pre-trained models\n",
        "save pre-trained checkpoints"
      ],
      "metadata": {
        "id": "rdW3TqwAnAmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wl8DZjOOqyEh",
        "outputId": "d533f7a6-685b-4c61-8521-e97dde91f527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2021.11.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.62.3)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.5)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-e5sak_f8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-e5sak_f8\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2021.11.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.62.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.16.0+cu121)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.26.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (1.26.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.6)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=2016adaad550c4f561eb24a93cdb9c57cb7275dfae41856eea17b40ae83c713a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-l986bwk0/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import clip.clip as clip\n",
        "import os\n",
        "import torch\n",
        "from collections import OrderedDict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIcusHuPnAJH",
        "outputId": "4e005bce-2a6e-49fb-8f29-8975166a0af4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (5.2.0)/charset_normalizer (2.0.9) doesn't match a supported version!\n",
            "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ..\n",
        "path = './clip_visual_encoder'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unt6tYsQnVew",
        "outputId": "98712af4-2409-46d0-8485-a4917d32c925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/1lKO9qUQLnjUauiyqzl6VVs2RkwHidE3c/unmasked teacher\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, _ = clip.load(\"ViT-B/16\", device='cpu')\n",
        "new_state_dict = OrderedDict()\n",
        "for k, v in model.state_dict().items():\n",
        "    if 'visual.' in k:\n",
        "        new_state_dict[k[7:]] = v\n",
        "torch.save(new_state_dict, os.path.join(path, 'vit_b16.pth'))"
      ],
      "metadata": {
        "id": "AqMuPdL2nZ9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, _ = clip.load(\"ViT-L/14\", device='cpu')\n",
        "new_state_dict = OrderedDict()\n",
        "for k, v in model.state_dict().items():\n",
        "    if 'visual.' in k:\n",
        "        new_state_dict[k[7:]] = v\n",
        "torch.save(new_state_dict, os.path.join(path, 'vit_l14.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZTqsJQ5rijm",
        "outputId": "45b85102-e2a8-45b7-9d63-4db944c254ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 890M/890M [00:06<00:00, 134MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, _ = clip.load(\"ViT-L/14@336px\", device='cpu')\n",
        "new_state_dict = OrderedDict()\n",
        "for k, v in model.state_dict().items():\n",
        "    if 'visual.' in k:\n",
        "        new_state_dict[k[7:]] = v\n",
        "torch.save(new_state_dict, os.path.join(path, 'vit_l14_336.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fGTQw6LrlPd",
        "outputId": "cfccaa8a-21ea-40b6-84f1-5c44b2a1f9bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 891M/891M [00:10<00:00, 88.3MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### define model"
      ],
      "metadata": {
        "id": "0Vw4YoAmoOIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fvcore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YclCxgenvvQt",
        "outputId": "8e6609a1-a8dd-4010-f510-3c85c87c77b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m947.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.22.4)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.62.3)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (9.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n",
            "Requirement already satisfied: iopath>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.1.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (4.5.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (2.5.1)\n",
            "Building wheels for collected packages: fvcore\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=ce972306bd64f50ae3f8b1c29309f994478fade7361489d185891ab51901e711\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "Successfully built fvcore\n",
            "Installing collected packages: yacs, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from fvcore.nn import FlopCountAnalysis\n",
        "from fvcore.nn import flop_count_table\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "WmmvknE8oNVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = './clip_visual_encoder'\n",
        "_MODELS = {\n",
        "    # extracted from OpenAI, see extract_clip\n",
        "    \"ViT-B/16\": os.path.join(MODEL_PATH, \"vit_b16.pth\"),\n",
        "    \"ViT-L/14\": os.path.join(MODEL_PATH, \"vit_l14.pth\"),\n",
        "    \"ViT-L/14_336\": os.path.join(MODEL_PATH, \"vit_l14_336.pth\"),\n",
        "}"
      ],
      "metadata": {
        "id": "sCmIC-k8oLxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.LayerNorm):\n",
        "    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        orig_type = x.dtype\n",
        "        ret = super().forward(x.type(torch.float32))\n",
        "        return ret.type(orig_type)\n",
        "\n",
        "\n",
        "class QuickGELU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(1.702 * x)\n",
        "\n",
        "\n",
        "class ResidualAttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_head, attn_mask=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attn = nn.MultiheadAttention(d_model, n_head)\n",
        "        self.ln_1 = LayerNorm(d_model)\n",
        "        self.mlp = nn.Sequential(OrderedDict([\n",
        "            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n",
        "            (\"gelu\", QuickGELU()),\n",
        "            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n",
        "        ]))\n",
        "        self.ln_2 = LayerNorm(d_model)\n",
        "        self.attn_mask = attn_mask\n",
        "\n",
        "    def attention(self, x, return_attn=False):\n",
        "        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n",
        "        if return_attn:\n",
        "            return self.attn(x, x, x, need_weights=True, attn_mask=self.attn_mask)\n",
        "        else:\n",
        "            return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n",
        "\n",
        "    def forward(self, x, return_attn=False):\n",
        "        if return_attn:\n",
        "            x_, attn = self.attention(self.ln_1(x), return_attn=True)\n",
        "            x = x + x_\n",
        "            x = x + self.mlp(self.ln_2(x))\n",
        "            return x, attn\n",
        "        else:\n",
        "            x = x + self.attention(self.ln_1(x))\n",
        "            x = x + self.mlp(self.ln_2(x))\n",
        "            return x\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "            self, width, layers, heads, return_attn=False,\n",
        "            clip_return_layer=1, clip_return_interval=1,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.return_attn = return_attn\n",
        "        self.resblocks = nn.ModuleList()\n",
        "        for _ in range(layers):\n",
        "            self.resblocks.append(\n",
        "                ResidualAttentionBlock(\n",
        "                    width, heads,\n",
        "                )\n",
        "            )\n",
        "        self.return_index = []\n",
        "        for i in range(clip_return_layer):\n",
        "            self.return_index.append(layers - int(i * clip_return_interval) - 1)\n",
        "        print(f'Teacher return index: {self.return_index}')\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn = None\n",
        "        z = []\n",
        "        for idx, blk in enumerate(self.resblocks):\n",
        "            if idx == self.layers - 1 and self.return_attn:\n",
        "                x, attn = blk(x, return_attn=True)\n",
        "            else:\n",
        "                x = blk(x)\n",
        "            if idx in self.return_index:\n",
        "                z.append(x)\n",
        "        x = torch.stack(z)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self, input_resolution, patch_size, width, layers, heads, output_dim,\n",
        "        clip_norm_type='l2', kernel_size=1,\n",
        "        return_attn=False, clip_return_layer=1, clip_return_interval=1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.clip_norm_type = clip_norm_type\n",
        "        self.return_attn = return_attn\n",
        "        print(f'Normalization Type: {clip_norm_type}')\n",
        "        print(f'Return Attention: {return_attn}')\n",
        "        print(f'Return Layer: {clip_return_layer}')\n",
        "        print(f'Return Interval: {clip_return_interval}')\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.conv1 = nn.Conv3d(\n",
        "            3, width,\n",
        "            (kernel_size, patch_size, patch_size),\n",
        "            (kernel_size, patch_size, patch_size),\n",
        "            (0, 0, 0), bias=False\n",
        "        )\n",
        "\n",
        "        scale = width ** -0.5\n",
        "        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n",
        "        self.positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size) ** 2 + 1, width))\n",
        "        self.ln_pre = LayerNorm(width)\n",
        "\n",
        "        self.transformer = Transformer(\n",
        "            width, layers, heads, return_attn=return_attn,\n",
        "            clip_return_layer=clip_return_layer,\n",
        "            clip_return_interval=clip_return_interval,\n",
        "        )\n",
        "\n",
        "        self.ln_post = LayerNorm(width)\n",
        "        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.conv1(x)  # shape = [*, width, grid, grid]\n",
        "        N, C, T, H, W = x.shape\n",
        "        x = x.permute(0, 2, 3, 4, 1).reshape(N * T, H * W, C)\n",
        "\n",
        "        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n",
        "        x = x + self.positional_embedding.to(x.dtype)\n",
        "        x = self.ln_pre(x)\n",
        "\n",
        "        if mask is not None:\n",
        "            cls_tokens = x[:, :1, :]\n",
        "            x = x[:, 1:]\n",
        "            x = x.reshape(N, T * H * W, C)\n",
        "            x = x[~mask].view(N * T, -1, C)\n",
        "            HW = x.shape[1]\n",
        "            x = torch.cat([cls_tokens, x], dim=1)\n",
        "        else:\n",
        "            HW = H * W\n",
        "\n",
        "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
        "        x, attn = self.transformer(x)\n",
        "\n",
        "        K = x.shape[0]\n",
        "        x = self.ln_post(x[:, 1:, :, :])  # [HW, NT, C]\n",
        "        x = x.view(K, HW, N, T, C).permute(0, 2, 3, 1, 4).reshape(K, N, T * HW, C)  # [K, N, THW, C]\n",
        "        x = x @ self.proj\n",
        "\n",
        "        if self.clip_norm_type == 'l2':\n",
        "            x = x / x.norm(dim=-1, keepdim=True)\n",
        "        elif self.clip_norm_type == 'none':\n",
        "            pass\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        if self.return_attn:\n",
        "            return x, attn[:, 0, 1:]\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "\n",
        "def inflate_weight(weight_2d, time_dim, center=True):\n",
        "    print(f'Init center: {center}')\n",
        "    if center:\n",
        "        weight_3d = torch.zeros(*weight_2d.shape)\n",
        "        weight_3d = weight_3d.unsqueeze(2).repeat(1, 1, time_dim, 1, 1)\n",
        "        middle_idx = time_dim // 2\n",
        "        weight_3d[:, :, middle_idx, :, :] = weight_2d\n",
        "    else:\n",
        "        weight_3d = weight_2d.unsqueeze(2).repeat(1, 1, time_dim, 1, 1)\n",
        "        weight_3d = weight_3d / time_dim\n",
        "    return weight_3d\n",
        "\n",
        "\n",
        "def load_state_dict(model, state_dict, input_resolution=224, patch_size=16, center=True):\n",
        "    state_dict_3d = model.state_dict()\n",
        "    for k in state_dict.keys():\n",
        "        if k in state_dict_3d.keys() and state_dict[k].shape != state_dict_3d[k].shape:\n",
        "            if len(state_dict_3d[k].shape) <= 2:\n",
        "                print(f'Ignore: {k}')\n",
        "                continue\n",
        "            print(f'Inflate: {k}, {state_dict[k].shape} => {state_dict_3d[k].shape}')\n",
        "            time_dim = state_dict_3d[k].shape[2]\n",
        "            state_dict[k] = inflate_weight(state_dict[k], time_dim, center=center)\n",
        "\n",
        "    pos_embed_checkpoint = state_dict['positional_embedding']\n",
        "    embedding_size = pos_embed_checkpoint.shape[-1]\n",
        "    num_patches = (input_resolution // patch_size) ** 2\n",
        "    orig_size = int((pos_embed_checkpoint.shape[-2] - 1) ** 0.5)\n",
        "    new_size = int(num_patches ** 0.5)\n",
        "    if orig_size != new_size:\n",
        "        print(f'Pos_emb from {orig_size} to {new_size}')\n",
        "        extra_tokens = pos_embed_checkpoint[:1]\n",
        "        pos_tokens = pos_embed_checkpoint[1:]\n",
        "        pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n",
        "        pos_tokens = torch.nn.functional.interpolate(\n",
        "            pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n",
        "        pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(0, 2)\n",
        "        new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=0)\n",
        "        state_dict['positional_embedding'] = new_pos_embed\n",
        "\n",
        "    model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "\n",
        "def clip_b16(\n",
        "    pretrained=True,\n",
        "    clip_norm_type='l2', input_resolution=224, kernel_size=1,\n",
        "    return_attn=False, center=True, clip_return_layer=1,\n",
        "    clip_return_interval=1\n",
        "):\n",
        "    model = VisionTransformer(\n",
        "        input_resolution=input_resolution, patch_size=16,\n",
        "        width=768, layers=12, heads=12, output_dim=512,\n",
        "        clip_norm_type=clip_norm_type,\n",
        "        kernel_size=kernel_size, return_attn=return_attn,\n",
        "        clip_return_layer=clip_return_layer,\n",
        "        clip_return_interval=clip_return_interval\n",
        "    )\n",
        "    if pretrained:\n",
        "        print('load pretrained weights')\n",
        "        state_dict = torch.load(_MODELS[\"ViT-B/16\"], map_location='cpu')\n",
        "        load_state_dict(model, state_dict, input_resolution=input_resolution, patch_size=16, center=center)\n",
        "    return model.eval()\n",
        "\n",
        "\n",
        "def clip_l14(\n",
        "    pretrained=True,\n",
        "    clip_norm_type='l2', input_resolution=224, kernel_size=1,\n",
        "    return_attn=False, center=True, clip_return_layer=1,\n",
        "    clip_return_interval=1\n",
        "):\n",
        "    model = VisionTransformer(\n",
        "        input_resolution=input_resolution, patch_size=14,\n",
        "        width=1024, layers=24, heads=16, output_dim=768,\n",
        "        clip_norm_type=clip_norm_type,\n",
        "        kernel_size=kernel_size, return_attn=return_attn,\n",
        "        clip_return_layer=clip_return_layer,\n",
        "        clip_return_interval=clip_return_interval\n",
        "    )\n",
        "    if pretrained:\n",
        "        print('load pretrained weights')\n",
        "        state_dict = torch.load(_MODELS[\"ViT-L/14\"], map_location='cpu')\n",
        "        load_state_dict(model, state_dict, input_resolution=input_resolution, patch_size=14, center=center)\n",
        "    return model.eval()\n",
        "\n",
        "\n",
        "def clip_l14_336(\n",
        "    pretrained=True,\n",
        "    clip_norm_type='l2', input_resolution=336, kernel_size=1,\n",
        "    return_attn=False, center=True, clip_return_layer=1,\n",
        "    clip_return_interval=1\n",
        "):\n",
        "    model = VisionTransformer(\n",
        "        input_resolution=input_resolution, patch_size=14,\n",
        "        width=1024, layers=24, heads=16, output_dim=768,\n",
        "        clip_norm_type=clip_norm_type,\n",
        "        kernel_size=kernel_size, return_attn=return_attn,\n",
        "        clip_return_layer=clip_return_layer,\n",
        "        clip_return_interval=clip_return_interval,\n",
        "    )\n",
        "    if pretrained:\n",
        "        print('load pretrained weights')\n",
        "        state_dict = torch.load(_MODELS[\"ViT-L/14_336\"], map_location='cpu')\n",
        "        load_state_dict(model, state_dict, input_resolution=input_resolution, patch_size=14, center=center)\n",
        "    return model.eval()"
      ],
      "metadata": {
        "id": "M0USQ6tNqAWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test dimension"
      ],
      "metadata": {
        "id": "gUev2yWWxXae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ./gdrive/MyDrive/dlcv_final/unmasked_teacher/\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zbEd9Nkx1sm",
        "outputId": "3d85f86a-8a09-4d23-b5f8-9cdf303803e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/1lKO9qUQLnjUauiyqzl6VVs2RkwHidE3c/unmasked teacher\n",
            "clip_visual_encoder  unmasked_teacher  unmasked_teacher.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 4217\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "num_frames = 8\n",
        "\n",
        "model = clip_b16(pretrained=True, kernel_size=1, return_attn=False, clip_return_layer=1)\n",
        "# print(model)\n",
        "\n",
        "print(model(torch.rand(1, 3, num_frames, 224, 224)).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSYKcJY3xbfO",
        "outputId": "c4efe2ec-14e7-49fa-91b0-ac53beba8751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalization Type: l2\n",
            "Return Attention: False\n",
            "Return Layer: 1\n",
            "Return Interval: 1\n",
            "Teacher return index: [11]\n",
            "load pretrained weights\n",
            "Inflate: conv1.weight, torch.Size([768, 3, 16, 16]) => torch.Size([768, 3, 1, 16, 16])\n",
            "Init center: True\n",
            "torch.Size([1, 1, 1568, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### UMT-QA backbone"
      ],
      "metadata": {
        "id": "OjprQ0e0-XBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uma_cfg"
      ],
      "metadata": {
        "id": "c1AkxpMnazT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LTnHrG8y-WiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-training\n"
      ],
      "metadata": {
        "id": "7m_Zs5WFyW0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./unmasked_teacher/single_modality/"
      ],
      "metadata": {
        "id": "_mqelq4rybEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GFg5rF_zykGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation\n",
        "functions defined in the repo"
      ],
      "metadata": {
        "id": "MfP98LYpqF9s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Util functions (from other files)"
      ],
      "metadata": {
        "id": "7pr6DXYqrKFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#def setup_main():"
      ],
      "metadata": {
        "id": "1lJ1eAkErJuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#def setup_seed"
      ],
      "metadata": {
        "id": "r5ZBtk2quHL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation function"
      ],
      "metadata": {
        "id": "LaFqRHA6rGXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''original file in vqa.py'''\n",
        "@torch.no_grad()\n",
        "def evaluation(model, data_loader, tokenizer, device, config):\n",
        "    model.eval()\n",
        "\n",
        "    metric_logger = MetricLogger(delimiter=\"  \")\n",
        "    header = \"[evaluation] Generating answers:\"\n",
        "    log_freq = config.log_freq // 2\n",
        "\n",
        "    result = []\n",
        "\n",
        "    raw_answer_list = data_loader.dataset.answer_list #list of answers to video questions\n",
        "    answer_list = [a + \" \" + config.eos for a in raw_answer_list]\n",
        "    answer_input = tokenizer(\n",
        "        answer_list,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=config.max_a_len,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "\n",
        "    logger.info(\"Start generating results.\")\n",
        "\n",
        "    #iterator = metric_logger.log_every(data_loader, log_freq, header)\n",
        "    iterator = data_loader\n",
        "    for n, data in enumerate(iterator):\n",
        "        #image, question, question_id = data\n",
        "        #use our dataset instead\n",
        "        video, candidates_QApair, gt_QApair, answer_text = data\n",
        "        video = video.to(device, non_blocking=True)\n",
        "        question_input = tokenizer(\n",
        "            #question,\n",
        "            candidates_QApair,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=config.max_q_len,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(device)\n",
        "\n",
        "        print(f\"evaluating question input: {candidate_QApair}, {question_input}\")\n",
        "\n",
        "        topk_ids, topk_probs = model(\n",
        "            video, question_input, answer_input, train=False, k=config.evaluation.k_test\n",
        "        )\n",
        "\n",
        "        for ques_id, topk_id, topk_prob in zip(question_id, topk_ids, topk_probs):\n",
        "            ques_id = int(ques_id.item()) if not isinstance(ques_id, str) else ques_id\n",
        "            _, pred = topk_prob.max(dim=0)\n",
        "            result.append({\"question_id\": ques_id, \"answer\": raw_answer_list[topk_id[pred]]})\n",
        "\n",
        "    print(f\"evaluation result: {result}\")\n",
        "    return result"
      ],
      "metadata": {
        "id": "-dMo87FVqK8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset function (might be redundant)"
      ],
      "metadata": {
        "id": "bFQzNOmdzhOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import ConcatDataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import InterpolationMode"
      ],
      "metadata": {
        "id": "C90kuN_E9M4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(dataset_type=\"qa_eval\", config):\n",
        "\n",
        "    # --- transforms --- #\n",
        "    # use clip as vision encoder\n",
        "    mean = (0.48145466, 0.4578275, 0.40821073)\n",
        "    std = (0.26862954, 0.26130258, 0.27577711)\n",
        "\n",
        "    '''\n",
        "    vision_enc_name = config.model.vision_encoder.name\n",
        "    if \"swin\" in vision_enc_name or \"vit\" in vision_enc_name:\n",
        "        mean = (0.485, 0.456, 0.406)\n",
        "        std = (0.229, 0.224, 0.225)\n",
        "    elif \"beit\" in vision_enc_name:\n",
        "        mean = (0.5, 0.5, 0.5)  # for all beit model except IN1K finetuning\n",
        "        std = (0.5, 0.5, 0.5)\n",
        "    elif \"clip\" in vision_enc_name:\n",
        "        mean = (0.48145466, 0.4578275, 0.40821073)\n",
        "        std = (0.26862954, 0.26130258, 0.27577711)\n",
        "    else:\n",
        "        raise ValueError\n",
        "    '''\n",
        "\n",
        "    normalize = transforms.Normalize(mean, std)\n",
        "\n",
        "    # loaded images and videos are torch.Tensor of torch.uint8 format,\n",
        "    # ordered as (T, 1 or 3, H, W) where T=1 for image\n",
        "    type_transform = transforms.Lambda(lambda x: x.float().div(255.0))\n",
        "\n",
        "    # use random augmentation\n",
        "    aug_transform = transforms.RandAugment()\n",
        "\n",
        "    '''\n",
        "    if config.inputs.video_input.random_aug:\n",
        "        aug_transform = transforms.RandAugment()\n",
        "    else:\n",
        "        aug_transform = transforms.Lambda(lambda x: x)\n",
        "    '''\n",
        "\n",
        "    train_transform = transforms.Compose(\n",
        "        [\n",
        "            aug_transform,\n",
        "            transforms.RandomResizedCrop(\n",
        "                inputs.image_res,\n",
        "                scale=(0.5, 1.0),\n",
        "                interpolation=InterpolationMode.BICUBIC,\n",
        "            ),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            type_transform,\n",
        "            normalize,\n",
        "        ]\n",
        "    )\n",
        "    test_transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(\n",
        "                (inputs.image_res, inputs.image_res),\n",
        "                interpolation=InterpolationMode.BICUBIC,\n",
        "            ),\n",
        "            type_transform,\n",
        "            normalize,\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "    # --- read in video --- #\n",
        "    # in QA dataset :\n",
        "    video_reader_type = \"decord\"\n",
        "\n",
        "    # video_reader_type = config.inputs.video_input.get(\"video_reader_type\", \"decord\")\n",
        "    video_only_dataset_kwargs_train = dict(\n",
        "        video_reader_type=video_reader_type,\n",
        "        sample_type=inputs.video_input.sample_type,\n",
        "        num_frames=inputs.video_input.num_frames,\n",
        "        num_tries=3,  # false tolerance\n",
        "    )\n",
        "    video_only_dataset_kwargs_eval = dict(\n",
        "        video_reader_type=video_reader_type,\n",
        "        sample_type=inputs.video_input.sample_type_test,\n",
        "        num_frames=inputs.video_input.num_frames_test,\n",
        "        num_tries=1,  # we want to have predictions for all videos\n",
        "    )\n",
        "\n",
        "    # can ignore these since our task is QA\n",
        "    '''\n",
        "    if dataset_type in [\"ret_train\", \"ret_eval\"]:  # for didemo and activitynet captions\n",
        "        is_paragraph_retrieval = config.get(\"is_paragraph_retrieval\", False)\n",
        "        video_only_dataset_kwargs_eval[\"is_paragraph_retrieval\"] = is_paragraph_retrieval\n",
        "        video_only_dataset_kwargs_train[\"is_paragraph_retrieval\"] = is_paragraph_retrieval\n",
        "\n",
        "    if dataset_type in [\"pt_train\", \"ret_train\"]:\n",
        "        # convert to list of lists\n",
        "        train_files = (\n",
        "            [config.train_file] if isinstance(config.train_file[0], str) else config.train_file\n",
        "        )\n",
        "        train_media_types = sorted(list({get_media_type(e) for e in train_files}))\n",
        "        if dataset_type == \"ret_train\":\n",
        "            assert (\n",
        "                len(train_media_types) == 1\n",
        "            ), f\"retrieval downstream should only have one media type, got {train_media_types}\"\n",
        "\n",
        "        train_datasets = []\n",
        "        for m in train_media_types:\n",
        "            dataset_cls = ImgTxtRetTrainDataset if m == \"image\" else VidTxtRetTrainDataset\n",
        "            if dataset_type == \"pt_train\":\n",
        "                dataset_cls = (\n",
        "                    SQLiteImgTxtRetTrainDataset\n",
        "                    if m == \"image\"\n",
        "                    else SQLiteVidTxtRetTrainDataset\n",
        "                )\n",
        "            # dataset of the same media_type will be mixed in a single Dataset object\n",
        "            _train_files = [e for e in train_files if get_media_type(e) == m]\n",
        "\n",
        "            if dataset_type == \"pt_train\":\n",
        "                datasets = []\n",
        "                for train_file in _train_files:\n",
        "                    dataset_kwargs = dict(\n",
        "                        ann_file=train_file,\n",
        "                        transform=train_transform,\n",
        "                        has_multi_vision_gt=config.get(\n",
        "                            \"has_multi_vision_gt\", False\n",
        "                        ),  # true for ssv2 ret\n",
        "                    )\n",
        "                    if m == \"video\":\n",
        "                        dataset_kwargs.update(video_only_dataset_kwargs_train)\n",
        "                    datasets.append(dataset_cls(**dataset_kwargs))\n",
        "                dataset = ConcatDataset(datasets)\n",
        "                train_datasets.append(dataset)\n",
        "            else:\n",
        "                dataset_kwargs = dict(\n",
        "                    ann_file=_train_files,\n",
        "                    transform=train_transform,\n",
        "                    has_multi_vision_gt=config.get(\n",
        "                        \"has_multi_vision_gt\", False\n",
        "                    ),  # true for ssv2 ret\n",
        "                    trimmed30=config.get(\n",
        "                        \"trimmed30\", False\n",
        "                    ), # use the first 30s for didemo\n",
        "                )\n",
        "                if m == \"video\":\n",
        "                    dataset_kwargs.update(video_only_dataset_kwargs_train)\n",
        "                dataset = dataset_cls(**dataset_kwargs)\n",
        "                train_datasets.append(dataset)\n",
        "        return train_datasets\n",
        "\n",
        "    elif dataset_type in [\"pt_eval\", \"ret_eval\"]:\n",
        "        test_datasets = []\n",
        "        test_dataset_names = []\n",
        "        # multiple test datasets, all separate\n",
        "        for name, data_cfg in config.test_file.items():\n",
        "            media_type = get_media_type(data_cfg)\n",
        "            test_dataset_cls = (\n",
        "                ImgTxtRetEvalDataset if media_type == \"image\" else VidTxtRetEvalDataset\n",
        "            )\n",
        "            test_dataset_names.append(name)\n",
        "            dataset_kwargs = dict(\n",
        "                ann_file=[data_cfg],\n",
        "                transform=test_transform,\n",
        "                has_multi_vision_gt=config.get(\n",
        "                    \"has_multi_vision_gt\", False\n",
        "                ),  # true for ssv2 ret\n",
        "                trimmed30=config.get(\n",
        "                    \"trimmed30\", False\n",
        "                ), # use the first 30s for didemo\n",
        "            )\n",
        "            if media_type == \"video\":\n",
        "                dataset_kwargs.update(video_only_dataset_kwargs_eval)\n",
        "            test_datasets.append(test_dataset_cls(**dataset_kwargs))\n",
        "        return test_datasets, test_dataset_names\n",
        "    '''\n",
        "\n",
        "    if dataset_type == \"qa_train\":\n",
        "        media_type = get_media_type(config.train_file[0])  # assuming single train media type\n",
        "        dataset_cls = ImageQADataset if media_type == \"image\" else VideoQADataset\n",
        "        dataset_kwargs = dict(\n",
        "            ann_file=config.train_file, transform=train_transform, eos=config.eos, mode=\"train\"\n",
        "        )\n",
        "        if media_type == \"video\":\n",
        "            dataset_kwargs.update(video_only_dataset_kwargs_train)\n",
        "        train_dataset = dataset_cls(**dataset_kwargs)\n",
        "        return train_dataset\n",
        "\n",
        "    elif dataset_type == \"qa_eval\":\n",
        "        test_datasets = []\n",
        "        test_dataset_names = []\n",
        "        # multiple test datasets, all separate\n",
        "        for name, data_cfg in config.test_file.items():\n",
        "            media_type = get_media_type(data_cfg)\n",
        "            test_dataset_cls = ImageQADataset if media_type == \"image\" else VideoQADataset\n",
        "            test_dataset_names.append(name)\n",
        "            dataset_kwargs = dict(\n",
        "                ann_file=[data_cfg],\n",
        "                transform=test_transform,\n",
        "                eos=config.eos,\n",
        "                mode=\"eval\",\n",
        "                answer_list=config.answer_list,\n",
        "            )\n",
        "            if media_type == \"video\":\n",
        "                dataset_kwargs.update(video_only_dataset_kwargs_eval)\n",
        "            test_datasets.append(test_dataset_cls(**dataset_kwargs))\n",
        "        return test_datasets, test_dataset_names\n",
        "\n",
        "    '''\n",
        "    elif dataset_type == \"mc_test\":\n",
        "        dataset_kwargs = dict(ann_file=[config.test_file.mc_test], transform=test_transform)\n",
        "        dataset_kwargs.update(video_only_dataset_kwargs_eval)\n",
        "        return VidTxtRetMCEvalDataset(**dataset_kwargs)\n",
        "    '''"
      ],
      "metadata": {
        "id": "QZxNJ03Mz2dC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_dataloaders(config, dataset_name=\"vqa\"):\n",
        "    logger.info(f\"Creating {dataset_name} QA datasets\")\n",
        "    #train_dataset = create_dataset(\"qa_train\", config)\n",
        "    #test_datasets, test_dataset_names = create_dataset(\"qa_eval\", config)\n",
        "    all_datasets = [train_dataset] + test_datasets\n",
        "\n",
        "    if config.distributed:\n",
        "        num_tasks = get_world_size()\n",
        "        global_rank = get_rank()\n",
        "        samplers = create_sampler(\n",
        "            all_datasets,\n",
        "            shuffles=[True] + [False] * len(test_datasets),\n",
        "            num_tasks=num_tasks,\n",
        "            global_rank=global_rank,\n",
        "        )\n",
        "    else:\n",
        "        samplers = [None] * len(all_datasets)\n",
        "\n",
        "    train_bsz = config.inputs.batch_size[train_dataset.media_type]\n",
        "    test_bsz_list = [config.inputs.batch_size_test[d.media_type] for d in test_datasets]\n",
        "    loaders = create_loader(\n",
        "        all_datasets,\n",
        "        samplers,\n",
        "        batch_size=[train_bsz] + test_bsz_list,\n",
        "        num_workers=[\n",
        "            config.num_workers,\n",
        "        ]\n",
        "        * len(all_datasets),\n",
        "        is_trains=[True] + [False] * len(test_datasets),\n",
        "        collate_fns=[vqa_collate_fn] + [None] * len(test_datasets),\n",
        "    )\n",
        "    train_loader, test_loaders = loaders[0], loaders[1:]\n",
        "    test_name2loaders = {k: v for k, v in zip(test_dataset_names, test_loaders)}\n",
        "    return train_loader, test_name2loaders"
      ],
      "metadata": {
        "id": "ABO560nHzkdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using our dataset"
      ],
      "metadata": {
        "id": "62HIDj3MGPyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgsttAKlNF0_",
        "outputId": "636093fa-3d0a-41e8-e9fa-86ad7ac0b706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.21.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vqa_collate_fn(batch):\n",
        "    #video, candidates_QApair, gt_QApair, qid, sampled_frames, answer_text\n",
        "    image_list, candidate_qa_list, gt_qa_list, answer_list, n = [], [], [], [], []\n",
        "    #video, candidates_QApair, gt_QApair, qid, sampled_frames, answer_text\n",
        "    for image, candidate_qa, gt_qa, qid, sampled_frames, answer in batch:\n",
        "        image_list.append(image)\n",
        "        candidate_qa_list.append(candidate_qa)\n",
        "        gt_qa_list.append(gt_qa)\n",
        "        answer_list += answer\n",
        "        n.append(len(answer))\n",
        "    return (\n",
        "        torch.stack(image_list, dim=0),\n",
        "        candidate_qa_list,\n",
        "        gt_qa_list,\n",
        "        answer_list,\n",
        "        n,\n",
        "    )"
      ],
      "metadata": {
        "id": "FeddD10lG9aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_vqa_dataloader():\n",
        "  json_file     = \"./dataset/STAR_train.json\"\n",
        "  video_folder  = \"./dataset/Charades_v1_480\"\n",
        "\n",
        "  train_dataset = VideoTextDataset(json_file, video_folder)\n",
        "  test_dataset  = VideoTextDataset(json_file, video_folder)\n",
        "\n",
        "  collate_fn = vqa_collate_fn()\n",
        "\n",
        "  train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=config.batch_size,\n",
        "            num_workers=2,\n",
        "            pin_memory=False,\n",
        "            #sampler=sampler,\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_fn,\n",
        "            #drop_last=drop_last,\n",
        "            persistent_workers=True if config.n_worker > 0 else False,\n",
        "        )\n",
        "  test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=config.batch_size,\n",
        "            num_workers=2,\n",
        "            pin_memory=False,\n",
        "            #sampler=sampler,\n",
        "            shuffle=False,\n",
        "            collate_fn=collate_fn,\n",
        "            #drop_last=drop_last,\n",
        "            persistent_workers=True if config.n_worker > 0 else False,\n",
        "        )\n",
        "\n",
        "  return train_loader, test_loader"
      ],
      "metadata": {
        "id": "LYZSSjunFyDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up model"
      ],
      "metadata": {
        "id": "TztSWSkJNpHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "from uma_models.umt_qa import UMT_QA\n",
        "import uma_cfg\n",
        "from uma_utils.optimizer import create_optimizer\n",
        "from uma_utils.scheduler import create_scheduler"
      ],
      "metadata": {
        "id": "NPdPjNk17pE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp"
      ],
      "metadata": {
        "id": "9woaRJ95Gi8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_vqa_model(\n",
        "    config, model_cls, has_decoder=False, pretrain=False, find_unused_parameters=False\n",
        "):\n",
        "    #logger.info(\"Creating model\")\n",
        "    #config = copy.deepcopy(config)\n",
        "    print(f\"setting up vqa model, with configuration: {config}\")\n",
        "\n",
        "    config.pretrained_path = './uma_checkpoints/qa_anet_b16_25m.pth'\n",
        "    '''\n",
        "    --- Text Encoder ---\n",
        "    TextEncoders[\"bert\"] = dict(\n",
        "    name=\"bert_base\",\n",
        "    pretrained=\"bert-base-uncased\",\n",
        "    config=\"configs/config_bert.json\",\n",
        "    d_model=768,\n",
        "    fusion_layer=9,\n",
        "    )\n",
        "    '''\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    '''\n",
        "    if \"bert\" in config.model[\"text_encoder\"][\"name\"]:\n",
        "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    else:\n",
        "        raise ValueError(f\"Not supported text encoder.\")\n",
        "    '''\n",
        "\n",
        "    # UMT-QA model\n",
        "    model = UMT_QA(config=config, tokenizer=tokenizer, is_pretrain=pretrain)\n",
        "\n",
        "    model = model.to(torch.device(config.device))\n",
        "    model_without_ddp = model\n",
        "\n",
        "    '''\n",
        "    --- For parallel computation ---\n",
        "    if config.distributed:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(\n",
        "            model,\n",
        "            device_ids=[config.gpu],\n",
        "            find_unused_parameters=find_unused_parameters,  # `False` for image-only task\n",
        "        )\n",
        "    '''\n",
        "\n",
        "    # optimizer & scheduler\n",
        "    optimizer = create_optimizer(config.optimizer, model)\n",
        "    scheduler = create_scheduler(config.scheduler, optimizer)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=config.fp16)\n",
        "\n",
        "    start_epoch = 0\n",
        "    global_step = 0\n",
        "\n",
        "    # auto resume the latest checkpoint\n",
        "    if config.get(\"auto_resume\", False):\n",
        "        logger.info(\"Auto resuming\")\n",
        "        model_latest = join(config.output_dir, \"ckpt_latest.pth\")\n",
        "        model_best = join(config.output_dir, \"ckpt_best.pth\")\n",
        "        large_num = -1\n",
        "        for p in os.listdir(config.output_dir):\n",
        "            if 'ckpt' in p:\n",
        "                num = p.split('_')[1].split('.')[0]\n",
        "                if str.isnumeric(num):\n",
        "                    if int(num) > large_num:\n",
        "                        large_num = int(num)\n",
        "        if large_num != -1:\n",
        "            model_latest = join(config.output_dir, f\"ckpt_{large_num:02d}.pth\")\n",
        "        if osp.isfile(model_latest):\n",
        "            config.pretrained_path = model_latest\n",
        "            config.resume = True\n",
        "        elif osp.isfile(model_best):\n",
        "            config.pretrained_path = model_best\n",
        "            config.resume = True\n",
        "        else:\n",
        "            logger.info(f\"Not found checkpoint in {config.output_dir}\")\n",
        "\n",
        "    if osp.isfile(config.pretrained_path):\n",
        "        checkpoint = torch.load(config.pretrained_path, map_location=\"cpu\")\n",
        "        if 'model' in checkpoint.keys():\n",
        "            state_dict = checkpoint[\"model\"]\n",
        "        else:\n",
        "            state_dict = checkpoint\n",
        "\n",
        "        if config.resume:\n",
        "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "            scheduler.load_state_dict(checkpoint[\"scheduler\"])\n",
        "            scaler.load_state_dict(checkpoint[\"scaler\"])\n",
        "            start_epoch = checkpoint[\"epoch\"] + 1\n",
        "            global_step = checkpoint[\"global_step\"]\n",
        "        elif not pretrain:  # downstream init from pretrained ckpt\n",
        "\n",
        "            # interpolate positional embeddings.\n",
        "            if \"vit\" in config.model.vision_encoder.name:\n",
        "                pass\n",
        "            else:\n",
        "                raise ValueError(\n",
        "                    f\" vision encoder: {config.model.vision_encoder.name} not implelented\"\n",
        "                )\n",
        "\n",
        "            # finetuning starts here ->\n",
        "            if not config.evaluate or config.get(\"zero_shot\", False):  # finetuning from a pretarined weights.\n",
        "                for key in list(state_dict.keys()):\n",
        "                    if \"bert\" in key:\n",
        "                        encoder_key = key.replace(\"bert.\", \"\")\n",
        "                        state_dict[encoder_key] = state_dict[key]\n",
        "                        if not has_decoder:\n",
        "                            del state_dict[key]\n",
        "\n",
        "                    # init text decoder as multimodal encoder (last 6 layers of model.text_encoder)\n",
        "                    # only for generation tasks like VQA\n",
        "                    if has_decoder and \"text_encoder\" in key:\n",
        "                        if \"layer\" in key:\n",
        "                            encoder_keys = key.split(\".\")\n",
        "                            layer_num = int(encoder_keys[4])\n",
        "                            if layer_num < config.model.text_encoder.fusion_layer:\n",
        "                                del state_dict[key]\n",
        "                                continue\n",
        "                            else:\n",
        "                                decoder_layer_num = layer_num - config.model.text_encoder.fusion_layer\n",
        "                                encoder_keys[4] = str(decoder_layer_num)\n",
        "                                encoder_key = \".\".join(encoder_keys)\n",
        "                        else:\n",
        "                            encoder_key = key\n",
        "                        decoder_key = encoder_key.replace(\"text_encoder\", \"text_decoder\")\n",
        "                        state_dict[decoder_key] = state_dict[key]\n",
        "                        del state_dict[key]\n",
        "\n",
        "        msg = model_without_ddp.load_state_dict(state_dict, strict=False)\n",
        "        logger.info(msg)\n",
        "        logger.info(f\"Loaded checkpoint from {config.pretrained_path}\")\n",
        "    else:\n",
        "        logger.warning(\"No pretrained checkpoint provided, training from scratch\")\n",
        "\n",
        "    return (\n",
        "        model,\n",
        "        model_without_ddp,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        scaler,\n",
        "        tokenizer,\n",
        "        start_epoch,\n",
        "        global_step,\n",
        "    )"
      ],
      "metadata": {
        "id": "DHVd6F6cNofY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### main & others"
      ],
      "metadata": {
        "id": "xRFD1lbdtmBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check current path\n",
        "%pwd"
      ],
      "metadata": {
        "id": "vIbsInpDuvtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import datetime\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "from os.path import join\n",
        "from shutil import copyfile\n",
        "\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.distributed as dist\n",
        "import wandb\n",
        "from omegaconf import OmegaConf"
      ],
      "metadata": {
        "id": "XpkjD_aytgGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REUNummRBHIl",
        "outputId": "7a9b8f5c-379a-4052-cb56-25beaec51ae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall transformers\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "id": "57dBPRhWANFc",
        "outputId": "b4a877c3-c9c8-486f-bdbd-d7e550c718b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.22.0\n",
            "Uninstalling transformers-4.22.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/transformers-cli\n",
            "    /usr/local/lib/python3.10/dist-packages/transformers-4.22.0.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/transformers/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled transformers-4.22.0\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2021.11.10)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.26.0)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
            "  Using cached tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.12.1\n",
            "    Uninstalling tokenizers-0.12.1:\n",
            "      Successfully uninstalled tokenizers-0.12.1\n",
            "Successfully installed tokenizers-0.15.0 transformers-4.36.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from dataset import (create_dataset, create_loader, create_sampler,\n",
        "#                     vqa_collate_fn)\n",
        "#from dataset.utils import sync_save_result\n",
        "from uma_models.umt_qa import UMT_QA\n",
        "#from tasks.shared_utils import setup_model\n",
        "#from tasks.vqa_utils import eval_qa_acc\n",
        "from uma_utils.basic_utils import (MetricLogger, SmoothedValue, save_json,\n",
        "                               setup_seed)\n",
        "#from utils.config import Config\n",
        "from uma_utils.config_utils import setup_main\n",
        "from uma_utils.distributed import get_rank, get_world_size, is_main_process\n",
        "from uma_utils.logger import log_dict_to_wandb, setup_wandb"
      ],
      "metadata": {
        "id": "YAq1VAuMul0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbd99da2-8d64-44ec-b046-00c49bdb47ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "EdeZFrC7tffa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(config):\n",
        "\n",
        "    # logger\n",
        "    if is_main_process() and config.wandb.enable:\n",
        "         run = setup_wandb(config)\n",
        "\n",
        "    logger.info(f\"train_file: {config.train_file}\")\n",
        "\n",
        "    setup_seed(config.seed + get_rank())\n",
        "    device = torch.device(config.device)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    # set dataloader\n",
        "    #train_loader, test_name2loaders = setup_dataloaders(config)\n",
        "    train_loader, test_loader = setup_vqa_dataloader()\n",
        "\n",
        "    # set scheduler\n",
        "    config.scheduler.num_training_steps = len(train_loader) * config.scheduler.epochs # <- need how many epoches?\n",
        "    config.scheduler.num_warmup_steps = len(train_loader) * config.scheduler.warmup_epochs\n",
        "\n",
        "    # set model\n",
        "    model_cls = eval(config.model.get('model_cls', 'UMT_QA')) # <- get model class : UMT_QA\n",
        "    (\n",
        "        model,\n",
        "        model_without_ddp,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        scaler,\n",
        "        tokenizer,\n",
        "        start_epoch,\n",
        "        global_step,\n",
        "    ) = setup_vqa_model(\n",
        "        config,\n",
        "        model_cls=model_cls,\n",
        "        has_decoder=True,\n",
        "        pretrain=False,\n",
        "        # find_unused_parameters=True,\n",
        "        find_unused_parameters=False,\n",
        "    )\n",
        "    if is_main_process() and config.wandb.enable:\n",
        "        wandb.watch(model)\n",
        "\n",
        "    best = 0\n",
        "    best_epoch = 0\n",
        "    has_gt = config.dataset_name != \"vqa\" or config.test_types[0] == \"minival\"\n",
        "\n",
        "    logger.info(\"Start \" + \"evaluation\" if config.evaluate else \"training\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(start_epoch, config.scheduler.epochs):\n",
        "\n",
        "        # ----- train mode (can ignore temporarily) ----- #\n",
        "        if not config.evaluate:\n",
        "            global_step = train(\n",
        "                model,\n",
        "                train_loader,\n",
        "                optimizer,\n",
        "                tokenizer,\n",
        "                epoch,\n",
        "                global_step,\n",
        "                device,\n",
        "                scheduler,\n",
        "                scaler,\n",
        "                config,\n",
        "            )\n",
        "\n",
        "        if config.get('no_test', False) and not config.evaluate:\n",
        "            if is_main_process():\n",
        "                save_obj = {\n",
        "                    \"model\": model_without_ddp.state_dict(),\n",
        "                    \"optimizer\": optimizer.state_dict(),\n",
        "                    \"scheduler\": scheduler.state_dict(),\n",
        "                    \"scaler\": scaler.state_dict(),\n",
        "                    \"config\": config,\n",
        "                    \"epoch\": epoch,\n",
        "                    \"global_step\": global_step,\n",
        "                }\n",
        "                torch.save(save_obj, join(config.output_dir, \"ckpt_best.pth\"))\n",
        "                best_epoch = epoch\n",
        "\n",
        "        # ----- evaluate ----- #\n",
        "        else:\n",
        "            with torch.cuda.amp.autocast(enabled=config.fp16):\n",
        "                eval_res = {}\n",
        "                pred_name2file = {}\n",
        "\n",
        "                '''\n",
        "                --- We only need to evaluate vqa task on one dataset ---\n",
        "                for test_name, test_loader in test_name2loaders.items():\n",
        "                    if test_name not in config.test_types:\n",
        "                        logger.info(\n",
        "                            f\"Skip eval {test_name} split. All test_types {config.test_types}\"\n",
        "                        )\n",
        "                        continue\n",
        "                    logger.info(f\"Evaluating {test_name} split...\")\n",
        "\n",
        "                    # evaluate qa result\n",
        "                    qa_result = evaluation(model, test_loader, tokenizer, device, config)\n",
        "\n",
        "                    # sync/gather results and eval\n",
        "                    pred_file, gathered_result = sync_save_result(\n",
        "                        qa_result, config.result_dir, f\"{test_name}_latest\"\n",
        "                    )\n",
        "                    pred_name2file[test_name] = pred_file\n",
        "                    if is_main_process() and has_gt:\n",
        "                        eval_res[test_name] = eval_qa_acc(\n",
        "                            test_loader.dataset.anno_list,\n",
        "                            gathered_result,\n",
        "                            is_vqa=config.dataset_name == \"vqa\",\n",
        "                        )\n",
        "                  '''\n",
        "                  # evaluate qa result\n",
        "                  qa_result = evaluation(model, test_loader, tokenizer, device, config)\n",
        "\n",
        "\n",
        "                  # sync/gather results and eval\n",
        "                  pred_file, gathered_result = sync_save_result(\n",
        "                      qa_result, config.result_dir, f\"{test_name}_latest\"\n",
        "                  )\n",
        "                  pred_name2file[test_name] = pred_file\n",
        "                  if is_main_process() and has_gt:\n",
        "                      eval_res[test_name] = eval_qa_acc(\n",
        "                          test_loader.dataset.anno_list,\n",
        "                          gathered_result,\n",
        "                          is_vqa=config.dataset_name == \"vqa\",\n",
        "                      )\n",
        "\n",
        "            if is_main_process():\n",
        "                if len(eval_res) > 0:\n",
        "                    logger.info(f\"eval_res {eval_res}\")\n",
        "\n",
        "                    if config.wandb.enable:\n",
        "                        for name, acc_dict in eval_res.items():\n",
        "                            log_dict_to_wandb(acc_dict, step=global_step, prefix=f\"{name}/\")\n",
        "\n",
        "                # --- save results --- #\n",
        "                if not config.evaluate and has_gt and eval_res[config.stop_key][\"overall\"] > best:\n",
        "                    save_obj = {\n",
        "                        \"model\": model_without_ddp.state_dict(),\n",
        "                        \"optimizer\": optimizer.state_dict(),\n",
        "                        \"scheduler\": scheduler.state_dict(),\n",
        "                        \"scaler\": scaler.state_dict(),\n",
        "                        \"config\": config,\n",
        "                        \"epoch\": epoch,\n",
        "                        \"global_step\": global_step,\n",
        "                    }\n",
        "                    for name, pred_file in pred_name2file.items():\n",
        "                        copyfile(pred_file, pred_file.replace(\"latest\", \"best\"))\n",
        "                    save_json(\n",
        "                        eval_res, join(config.output_dir, \"eval_res_best.json\"), save_pretty=True\n",
        "                    )\n",
        "                    torch.save(save_obj, join(config.output_dir, \"ckpt_best.pth\"))\n",
        "                    best = eval_res[config.stop_key][\"overall\"]\n",
        "                    best_epoch = epoch\n",
        "\n",
        "                # -- save evaluation result -- #\n",
        "                if config.evaluate:\n",
        "                    save_json(\n",
        "                        eval_res, join(config.output_dir, \"eval_res_best.json\"), save_pretty=True\n",
        "                    )\n",
        "\n",
        "                    for name, pred_file in pred_name2file.items():\n",
        "                        copyfile(pred_file, pred_file.replace(\"latest\", \"eval\"))\n",
        "                        if has_gt:\n",
        "                            save_path = join(config.output_dir, f\"{name}_acc_eval.json\")\n",
        "                            save_json(eval_res[name], save_path, save_pretty=True)\n",
        "\n",
        "        if config.evaluate or config.debug:\n",
        "            break\n",
        "\n",
        "        dist.barrier()\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "    logger.info(f\"Training time {total_time_str}\")\n",
        "    logger.info(f\"best epoch {best_epoch}\")\n",
        "    logger.info(f\"Checkpoints and Logs saved at {config.output_dir}\")\n",
        "\n",
        "    if is_main_process() and config.wandb.enable:\n",
        "        run.finish()\n",
        "\n",
        "    # --- end of main process --- #"
      ],
      "metadata": {
        "id": "gEC6WsVOru_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_after_training(train_config):\n",
        "    # general config for all\n",
        "    train_config.wandb.enable = False\n",
        "    train_config.evaluate = True\n",
        "    train_config.pretrained_path = join(train_config.output_dir, \"ckpt_best.pth\")\n",
        "\n",
        "    if train_config.get('num_frames_test_final', False):\n",
        "        train_config.num_frames_test = train_config.num_frames_test_final\n",
        "        train_config.batch_size = train_config.batch_size_final\n",
        "        train_config.inputs.video_input.num_frames_test = train_config.num_frames_test_final\n",
        "        train_config.model.vision_encoder.num_frames = train_config.num_frames_test_final\n",
        "\n",
        "    eval_config = copy.deepcopy(train_config)\n",
        "    eval_config.test_types = (\n",
        "        [\"val\", \"test\"] if eval_config.dataset_name != \"vqa\" else [\"minival\"]\n",
        "    )\n",
        "    eval_config.output_dir = join(eval_config.output_dir, f\"eval_after_training\")\n",
        "    eval_config.result_dir = eval_config.output_dir\n",
        "    if is_main_process():\n",
        "        os.makedirs(eval_config.output_dir, exist_ok=False)\n",
        "        Config.dump(eval_config, os.path.join(eval_config.output_dir, \"config.json\"))\n",
        "    logger.info(f\"===========> START eval_after_training [{eval_config.test_types}]\")\n",
        "    main(eval_config)"
      ],
      "metadata": {
        "id": "aQfHaPxaroSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Main"
      ],
      "metadata": {
        "id": "jZT6vVq4qvPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZ7NLhncwiPU",
        "outputId": "251cc082-fe22-4cb8-e8e1-7ef77596025d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clip_visual_encoder  __pycache__  uma_checkpoints  uma_utils\t\t   unmasked_teacher_repo\n",
            "dataset\t\t     uma_cfg.py   uma_models\t   unmasked_teacher.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import uma_cfg\n",
        "config = uma_cfg\n",
        "print(config.dataset_name)"
      ],
      "metadata": {
        "id": "aaFEo11JrpT-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12dc2e3b-8135-415c-80e0-407250b8be18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "charades\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "FsJCmjAv2-7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# logger\n",
        "'''\n",
        "if is_main_process() and config.wandb[\"enable\"]:\n",
        "      run = setup_wandb(config)\n",
        "'''\n",
        "\n",
        "#logger.info(f\"train_file: {config.train_file}\")\n",
        "\n",
        "setup_seed(config.seed + get_rank())\n",
        "device = torch.device('cuda')\n",
        "cudnn.benchmark = True\n",
        "\n",
        "# set dataloader\n",
        "# train_loader, test_name2loaders = setup_dataloaders(config)\n",
        "# train_loader, test_loader = setup_vqa_dataloader()\n",
        "json_file     = \"./dataset/STAR_train.json\"\n",
        "video_folder  = \"./dataset/Charades_v1_480\"\n",
        "\n",
        "train_dataset = VideoTextDataset(json_file, video_folder)\n",
        "test_dataset  = VideoTextDataset(json_file, video_folder)"
      ],
      "metadata": {
        "id": "Rspful0ewnE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#collate_fn = vqa_collate_fn(config.batch_size)\n",
        "train_loader = DataLoader(\n",
        "          train_dataset,\n",
        "          batch_size=config.batch_size,\n",
        "          num_workers=2,\n",
        "          pin_memory=False,\n",
        "          #sampler=sampler,\n",
        "          shuffle=True,\n",
        "          collate_fn=vqa_collate_fn,\n",
        "          #drop_last=drop_last,\n",
        "          persistent_workers=True if config.num_workers > 0 else False,\n",
        "      )\n",
        "test_loader = DataLoader(\n",
        "          test_dataset,\n",
        "          batch_size=config.batch_size,\n",
        "          num_workers=2,\n",
        "          pin_memory=False,\n",
        "          #sampler=sampler,\n",
        "          shuffle=False,\n",
        "          collate_fn=vqa_collate_fn,\n",
        "          #drop_last=drop_last,\n",
        "          persistent_workers=True if config.num_workers > 0 else False,\n",
        "      )"
      ],
      "metadata": {
        "id": "RsF1swu2FJxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set scheduler\n",
        "config.scheduler[\"num_training_steps\"] = len(train_loader) * config.scheduler[\"epochs\"] # <- need how many epoches?\n",
        "config.scheduler[\"num_warmup_steps\"] = len(train_loader) * config.scheduler[\"warmup_epochs\"]"
      ],
      "metadata": {
        "id": "nK2TI7vN89w5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n, data in enumerate(test_loader):\n",
        "  print(data)"
      ],
      "metadata": {
        "id": "LyQAMSugIYqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set model\n",
        "model_cls = eval(config.model.get('model_cls', 'UMT_QA')) # <- get model class : UMT_QA\n",
        "(\n",
        "    model,\n",
        "    model_without_ddp,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    scaler,\n",
        "    tokenizer,\n",
        "    start_epoch,\n",
        "    global_step,\n",
        ") = setup_vqa_model(\n",
        "    config,\n",
        "    model_cls=model_cls,\n",
        "    has_decoder=True,\n",
        "    pretrain=False,\n",
        "    # find_unused_parameters=True,\n",
        "    find_unused_parameters=False,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "80Xt1OXpF9OZ",
        "outputId": "44e7094b-c835-4a74-e795-40ac0db94ee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "setting up vqa model, with configuration: <module 'uma_cfg' from '/content/gdrive/.shortcut-targets-by-id/1lKO9qUQLnjUauiyqzl6VVs2RkwHidE3c/unmasked teacher/uma_cfg.py'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-953d871a68ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_vqa_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmodel_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_cls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-a3bf5f67a658>\u001b[0m in \u001b[0;36msetup_vqa_model\u001b[0;34m(config, model_cls, has_decoder, pretrain, find_unused_parameters)\u001b[0m\n\u001b[1;32m     18\u001b[0m     '''\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     '''\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"bert\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_encoder\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BertTokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(config.model[\"vision_encoder\"][\"d_model\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9AKlkcxxd0I",
        "outputId": "fcb7a8f5-3243-4b8e-db67-eaf58190c444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'int'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For model load debugging"
      ],
      "metadata": {
        "id": "C3j_hftL-6bO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "import torch\n",
        "from einops import rearrange\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from uma_models.backbones.vit import build_vit, build_clip\n",
        "from uma_models.backbones.bert.builder import build_bert, build_bert_decoder\n",
        "from uma_models.criterions import MLMLoss, VTC_VTM_Loss, UTA_Loss\n",
        "from uma_models.mask import (\n",
        "    TubeMaskingGenerator,\n",
        "    RandomMaskingGenerator\n",
        ")"
      ],
      "metadata": {
        "id": "253xaFWQ-9Yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###UMT"
      ],
      "metadata": {
        "id": "1HJ6bVl8_icg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UMT_(nn.Module):\n",
        "    \"\"\"docstring for UMT\"\"\"\n",
        "\n",
        "    def __init__(self, config, tokenizer=None, is_pretrain=True):\n",
        "        super(UMT_, self).__init__()\n",
        "\n",
        "        self.config = config\n",
        "        self.tokenizer = tokenizer\n",
        "        self.text_width = config.model[\"text_encoder\"][\"d_model\"]\n",
        "\n",
        "        self.is_pretrain = is_pretrain\n",
        "        self.vision_width = 768\n",
        "        self.embed_dim = config.model[\"embed_dim\"]\n",
        "\n",
        "        # create modules.\n",
        "        self.vision_encoder = self.build_vision_encoder()\n",
        "        self.text_encoder = self.build_text_encoder()\n",
        "\n",
        "        self.vision_proj = nn.Linear(self.vision_width, self.embed_dim)\n",
        "        self.text_proj = nn.Linear(self.text_width, self.embed_dim)\n",
        "\n",
        "        self.temp = nn.parameter.Parameter(torch.ones([]) * config.model[\"temp\"])\n",
        "        self.itm_head = nn.Linear(self.text_width, 2)\n",
        "\n",
        "        # criterions\n",
        "        self.loss_weight = config.criterion[\"loss_weight\"]\n",
        "        self.criterion_uta = UTA_Loss(\n",
        "            config.criterion[\"uta_norm_type\"],\n",
        "            config.criterion[\"uta_loss_type\"],\n",
        "        )\n",
        "        self.criterion_vtc_vtm = VTC_VTM_Loss(config.criterion[\"vtm_hard_neg\"])\n",
        "        self.criterion_mlm = MLMLoss(config.criterion[\"mlm_masking_prob\"], tokenizer)\n",
        "\n",
        "    def forward(self, image, text, idx):\n",
        "        \"\"\"forward and calculate loss.\n",
        "\n",
        "        Args:\n",
        "            image (torch.Tensor): The input images. Shape: [B,T,C,H,W].\n",
        "            text (dict): TODO\n",
        "            idx (torch.Tensor): TODO\n",
        "\n",
        "        Returns: TODO\n",
        "\n",
        "        \"\"\"\n",
        "        self.clip_contrastive_temperature()\n",
        "\n",
        "        vision_embeds, pooled_vision_embeds, student_output, clip_output = self.encode_vision(image)\n",
        "        text_embeds, pooled_text_embeds = self.encode_text(text)\n",
        "\n",
        "        # obtain vision and text representations.\n",
        "        vision_proj = self.vision_proj(pooled_vision_embeds)\n",
        "        text_proj = self.text_proj(pooled_text_embeds)\n",
        "\n",
        "        # calculate loss\n",
        "        ## MCA loss\n",
        "        if self.loss_weight.uta != 0:\n",
        "            loss_uta = self.criterion_uta.uta_loss(student_output, clip_output)\n",
        "        else:\n",
        "            loss_uta = torch.tensor(0)\n",
        "\n",
        "        ## VTC loss\n",
        "        if self.loss_weight.vtc != 0:\n",
        "            loss_vtc = self.criterion_vtc_vtm.vtc_loss(\n",
        "                vision_proj, text_proj, idx, self.temp, all_gather=True\n",
        "            )\n",
        "        else:\n",
        "            loss_vtc = torch.tensor(0)\n",
        "\n",
        "        ## VTM loss\n",
        "        if self.loss_weight.vtm != 0:\n",
        "            loss_vtm = self.criterion_vtc_vtm.vtm_loss(\n",
        "                self.get_text_encoder(),\n",
        "                self.itm_head,\n",
        "                self.temp,\n",
        "                vision_embeds,\n",
        "                text_embeds,\n",
        "                vision_proj,\n",
        "                text_proj,\n",
        "                text.attention_mask,\n",
        "                idx,\n",
        "            )\n",
        "        else:\n",
        "            loss_vtm = torch.tensor(0)\n",
        "\n",
        "        ## MLM loss\n",
        "        if self.is_pretrain and self.loss_weight.mlm != 0:\n",
        "            loss_mlm = self.criterion_mlm.mlm_loss(\n",
        "                self.text_encoder, text, vision_embeds, None\n",
        "            )\n",
        "        else:\n",
        "            loss_mlm = torch.tensor(0)\n",
        "\n",
        "        return dict(\n",
        "            loss_uta=loss_uta * self.loss_weight.uta,\n",
        "            loss_vtc=loss_vtc * self.loss_weight.vtc,\n",
        "            loss_vtm=loss_vtm * self.loss_weight.vtm,\n",
        "            loss_mlm=loss_mlm * self.loss_weight.mlm,\n",
        "        )\n",
        "\n",
        "    def encode_teacher(self, image):\n",
        "        \"\"\"encode image / videos as features.\n",
        "\n",
        "        Args:\n",
        "            image (torch.Tensor): The input images.\n",
        "\n",
        "        Returns: tuple.\n",
        "            - mask (torch.Tensor): Mask. Shape: [B,N1].\n",
        "            - clip_output (torch.Tensor): The features of clip. Shape: [K,B,N,C].\n",
        "\n",
        "        \"\"\"\n",
        "        B, C, T, H, W = image.shape\n",
        "        mask_type = self.image_mask_type if T == 1 else self.video_mask_type\n",
        "        window_size = self.image_window_size if T == 1 else self.video_window_size\n",
        "        mask_ratio = self.image_mask_ratio if T == 1 else self.video_mask_ratio\n",
        "\n",
        "        if self.clip_teacher is None or self.loss_weight.uta == 0:\n",
        "            return None, None\n",
        "\n",
        "        if H != self.clip_img_size:\n",
        "            image = torch.nn.functional.interpolate(\n",
        "                image.reshape(B, C*T, H, W),\n",
        "                size=(self.clip_img_size, self.clip_img_size),\n",
        "                mode='bicubic', align_corners=False\n",
        "            )\n",
        "            image = image.view(B, C, T, self.clip_img_size, self.clip_img_size)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if mask_type == 'tube':\n",
        "                mask = TubeMaskingGenerator(window_size, mask_ratio, B)\n",
        "                clip_output, attn = self.clip_teacher(image)\n",
        "            elif mask_type == 'random':\n",
        "                mask = RandomMaskingGenerator(window_size, mask_ratio, B)\n",
        "                clip_output, attn = self.clip_teacher(image)\n",
        "            elif mask_type in 'attention':\n",
        "                clip_output, attn = self.clip_teacher(image)\n",
        "                BT, N = attn.shape\n",
        "                N_vis = N - int(N * mask_ratio)\n",
        "                importance = torch.multinomial(attn, N)\n",
        "                mask = torch.ones((BT, N))\n",
        "                pos1 = torch.arange(BT).view(-1, 1).repeat(1, N_vis)\n",
        "                pos2 = importance[:, :N_vis]\n",
        "                mask[pos1, pos2] = 0\n",
        "                mask = mask.view(B, -1).to(torch.bool)\n",
        "            else:\n",
        "                raise NotImplementedError\n",
        "\n",
        "            # mask clip output\n",
        "            K, _, _, C = clip_output.shape\n",
        "            mask_clip = mask.unsqueeze(0).repeat(K, 1, 1)\n",
        "            clip_output = clip_output[~mask_clip].reshape(K, B, -1, C)\n",
        "\n",
        "        return mask, clip_output\n",
        "\n",
        "    def encode_vision(self, image, test=False):\n",
        "        \"\"\"encode image / videos as features.\n",
        "\n",
        "        Args:\n",
        "            image (torch.Tensor): The input images.\n",
        "            test (bool): Whether testing.\n",
        "\n",
        "        Returns: tuple.\n",
        "            - vision_embeds (torch.Tensor): The output features. Shape: [B,N,C].\n",
        "            - pooled_vision_embeds (torch.Tensor): The pooled output features. Shape: [B,1,C].\n",
        "            - student_output (torch.Tensor): The features of alignment. Shape: [K,B,N,C].\n",
        "            - clip_output (torch.Tensor): The features of clip. Shape: [K,B,N,C].\n",
        "\n",
        "        \"\"\"\n",
        "        T = image.shape[1]\n",
        "        use_image = True if T == 1 else False\n",
        "        image = image.permute(0, 2, 1, 3, 4) # [B,T,C,H,W] -> [B,C,T,H,W]\n",
        "        # whether save temporal dimension\n",
        "        keep_temporal=self.config.model.vision_encoder.keep_temporal\n",
        "        if test:\n",
        "            vision_embeds, pooled_vision_embeds, _ = self.vision_encoder(\n",
        "                image, None, use_image, keep_temporal,\n",
        "            )\n",
        "            return vision_embeds, pooled_vision_embeds\n",
        "        else:\n",
        "            mask, clip_output = self.encode_teacher(image)\n",
        "            if mask is not None and (self.video_mask_type != 'tube' or self.image_mask_type != 'tube'):\n",
        "                keep_temporal = False\n",
        "            vision_embeds, pooled_vision_embeds, student_output = self.vision_encoder(\n",
        "                image, mask, use_image, keep_temporal\n",
        "            )\n",
        "            return vision_embeds, pooled_vision_embeds, student_output, clip_output\n",
        "\n",
        "    def encode_text(self, text):\n",
        "        \"\"\"encode text.\n",
        "        Args:\n",
        "            text (dict): The output of huggingface's `PreTrainedTokenizer`. contains keys:\n",
        "                - input_ids (torch.Tensor): Token ids to be fed to a model. Shape: [B,L].\n",
        "                - attention_mask (torch.Tensor): The mask indicate padded tokens. Shape: [B,L]. 0 is padded token.\n",
        "                - other keys refer to \"https://huggingface.co/docs/transformers/v4.21.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__\".\n",
        "        Returns: tuple.\n",
        "            - text_embeds (torch.Tensor): The features of all tokens. Shape: [B,L,C].\n",
        "            - pooled_text_embeds (torch.Tensor): The pooled features. Shape: [B,C].\n",
        "\n",
        "        \"\"\"\n",
        "        text_output = self.get_text_encoder()(\n",
        "            text.input_ids,\n",
        "            attention_mask=text.attention_mask,\n",
        "            return_dict=True,\n",
        "            mode=\"text\",\n",
        "        )\n",
        "        text_embeds = text_output.last_hidden_state\n",
        "        pooled_text_embeds = text_embeds[:, 0]\n",
        "        return text_embeds, pooled_text_embeds\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def clip_contrastive_temperature(self, min_val=0.001, max_val=0.5):\n",
        "        \"\"\"Seems only used during pre-training\"\"\"\n",
        "        self.temp.clamp_(min_val, max_val)\n",
        "\n",
        "    def build_vision_encoder(self):\n",
        "        \"\"\"build vision encoder\n",
        "        Returns: (vision_encoder, clip_teacher). Each is a `nn.Module`.\n",
        "\n",
        "        \"\"\"\n",
        "        encoder_name = self.config.model.vision_encoder.name\n",
        "        print(f\"Build vision_encoder: {encoder_name}\")\n",
        "        if \"vit\" in encoder_name:\n",
        "            vision_encoder = build_vit(self.config.model)\n",
        "        else:\n",
        "            raise ValueError(f\"not implemented: {encoder_name}\")\n",
        "\n",
        "        teacher_name = self.config.model.vision_encoder.clip_teacher\n",
        "        self.clip_teacher = None\n",
        "        if teacher_name != 'none':\n",
        "            self.clip_teacher = build_clip(self.config.model)\n",
        "        # parameters for mask\n",
        "        img_size = self.config.model[\"vision_encoder\"][\"img_size\"]\n",
        "        '''\n",
        "        num_frames = self.config.model.vision_encoder.num_frames\n",
        "        tublet_size = self.config.model.vision_encoder.tubelet_size\n",
        "        patch_size = self.config.model.vision_encoder.patch_size\n",
        "        self.clip_img_size = self.config.model.vision_encoder.clip_img_size\n",
        "        self.video_mask_type = self.config.model.vision_encoder.video_mask_type\n",
        "        self.video_window_size = (num_frames // tublet_size, img_size // patch_size, img_size // patch_size)\n",
        "        self.video_mask_ratio = self.config.model.vision_encoder.video_mask_ratio\n",
        "        self.image_mask_type = self.config.model.vision_encoder.image_mask_type\n",
        "        self.image_window_size = (1, img_size // patch_size, img_size // patch_size)\n",
        "        self.image_mask_ratio = self.config.model.vision_encoder.image_mask_ratio\n",
        "        '''\n",
        "\n",
        "        num_frames = self.config.model['vision_encoder']['num_frames']\n",
        "        tublet_size = self.config.model['vision_encoder']['tubelet_size']\n",
        "        patch_size = self.config.model['vision_encoder']['patch_size']\n",
        "        self.clip_img_size = self.config.model['vision_encoder']['clip_img_size']\n",
        "        self.video_mask_type = self.config.model['vision_encoder']['video_mask_type']\n",
        "        self.video_window_size = (num_frames // tublet_size, img_size // patch_size, img_size // patch_size)\n",
        "        self.video_mask_ratio = self.config.model['vision_encoder']['video_mask_ratio']\n",
        "        self.image_mask_type = self.config.model['vision_encoder']['image_mask_type']\n",
        "        self.image_window_size = (1, img_size // patch_size, img_size // patch_size)\n",
        "        self.image_mask_ratio = self.config.model['vision_encoder']['image_mask_ratio']\n",
        "\n",
        "        return vision_encoder\n",
        "\n",
        "    def build_text_encoder(self):\n",
        "        \"\"\"build text_encoder and possiblly video-to-text multimodal fusion encoder.\n",
        "        Returns: nn.Module. The text encoder\n",
        "\n",
        "        \"\"\"\n",
        "        encoder_name = self.config.model[\"text_encoder\"][\"name\"]\n",
        "        logger.info(f\"Build text_encoder {encoder_name}\")\n",
        "\n",
        "        if \"bert\" in encoder_name:\n",
        "            text_encoder = build_bert(\n",
        "                self.config.model,\n",
        "                self.is_pretrain,\n",
        "                self.config.gradient_checkpointing,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Not implemented: {encoder_name}\")\n",
        "\n",
        "        return text_encoder\n",
        "\n",
        "    def get_text_encoder(self):\n",
        "        \"\"\"get text encoder, used for text and cross-modal encoding\"\"\"\n",
        "        encoder = self.text_encoder\n",
        "        return encoder.bert if hasattr(encoder, \"bert\") else encoder\n"
      ],
      "metadata": {
        "id": "VFA1K0op_HM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###UMT_QA"
      ],
      "metadata": {
        "id": "mzC2L48Y_kyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UMT_QA_(UMT_):\n",
        "    \"\"\"docstring for UMT_QA\"\"\"\n",
        "\n",
        "    def __init__(self, config, tokenizer, is_pretrain=False):\n",
        "        super(UMT_QA_, self).__init__(config, tokenizer, is_pretrain)\n",
        "\n",
        "        # delete extra/unnecessary modules inherited from SingularityRetrievalBase\n",
        "        extra_attributes = [\"vision_proj\", \"text_proj\", \"temp\", \"itm_head\"]\n",
        "        for attr in extra_attributes:\n",
        "            delattr(self, attr)\n",
        "\n",
        "        self.text_decoder = self.build_text_decoder()\n",
        "\n",
        "    def build_vision_encoder(self):\n",
        "        \"\"\"build vision encoder\n",
        "        Returns: (vision_encoder, clip_teacher). Each is a `nn.Module`.\n",
        "\n",
        "        \"\"\"\n",
        "        encoder_name = self.config.model[\"vision_encoder\"][\"name\"]\n",
        "        print(f\"Build vision_encoder: {encoder_name}\")\n",
        "        if \"vit\" in encoder_name:\n",
        "            vision_encoder = build_vit(self.config.model, add_pool_norm=False)\n",
        "        else:\n",
        "            raise ValueError(f\"not implemented: {encoder_name}\")\n",
        "        return vision_encoder\n",
        "\n",
        "    def build_text_decoder(self):\n",
        "        encoder_name = self.config.model[\"text_encoder\"][\"name\"]\n",
        "        print(f\"Build text_decoder {encoder_name}\")\n",
        "        if \"bert\" in encoder_name:\n",
        "            text_decoder = build_bert_decoder(\n",
        "                self.config.model, self.config.gradient_checkpointing\n",
        "            )\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "        return text_decoder\n",
        "\n",
        "    def encode_vision(self, image):\n",
        "        \"\"\"encode image / videos as features.\n",
        "\n",
        "        Args:\n",
        "            image (torch.Tensor): The input images.\n",
        "            test (bool): Whether testing.\n",
        "\n",
        "        Returns: tuple.\n",
        "            - vision_embeds (torch.Tensor): The output features. Shape: [B,N,C].\n",
        "\n",
        "        \"\"\"\n",
        "        T = image.shape[1]\n",
        "        use_image = True if T == 1 else False\n",
        "        image = image.permute(0, 2, 1, 3, 4) # [B,T,C,H,W] -> [B,C,T,H,W]\n",
        "        # whether save temporal dimension\n",
        "        keep_temporal = self.config.model[\"vision_encoder\"][\"keep_temporal\"]\n",
        "        vision_embeds, _ = self.vision_encoder(\n",
        "            image, None, use_image\n",
        "        )\n",
        "\n",
        "        return vision_embeds\n",
        "\n",
        "    def forward(self, image, question, answer=None, k=None, weights=None, train=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "        k: number of answers for each question\n",
        "        weights: weight for each answer\n",
        "        \"\"\"\n",
        "        image_embeds = self.encode_vision(image)\n",
        "        print(f\"dimension of image(video) embedding: {image_embeds.size()}\")\n",
        "        print(f\"image embed: {image_embeds}\")\n",
        "        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image.device)\n",
        "\n",
        "        if train:\n",
        "            answer_targets = answer.input_ids.masked_fill(\n",
        "                answer.input_ids == self.tokenizer.pad_token_id, -100\n",
        "            )\n",
        "\n",
        "            question_output = self.text_encoder(\n",
        "                question.input_ids,\n",
        "                attention_mask=question.attention_mask,\n",
        "                encoder_hidden_states=image_embeds,\n",
        "                encoder_attention_mask=image_atts,\n",
        "                return_dict=True,\n",
        "            )\n",
        "\n",
        "            question_states = []\n",
        "            question_atts = []\n",
        "            for b, n in enumerate(k):\n",
        "                question_states += [question_output.last_hidden_state[b]] * n\n",
        "                question_atts += [question.attention_mask[b]] * n\n",
        "            question_states = torch.stack(question_states, 0)\n",
        "            question_atts = torch.stack(question_atts, 0)\n",
        "\n",
        "            answer_output = self.text_decoder(\n",
        "                answer.input_ids,\n",
        "                attention_mask=answer.attention_mask,\n",
        "                encoder_hidden_states=question_states,\n",
        "                encoder_attention_mask=question_atts,\n",
        "                labels=answer_targets,\n",
        "                return_dict=True,\n",
        "                reduction=\"none\",\n",
        "            )\n",
        "            loss = weights * answer_output.loss\n",
        "            loss = loss.sum() / image.size(0)\n",
        "\n",
        "            return loss\n",
        "\n",
        "        # inference\n",
        "        else:\n",
        "            question_output = self.text_encoder(\n",
        "                question.input_ids,\n",
        "                attention_mask=question.attention_mask,\n",
        "                encoder_hidden_states=image_embeds,\n",
        "                encoder_attention_mask=image_atts,\n",
        "                return_dict=True,\n",
        "            )\n",
        "            topk_ids, topk_probs = self.rank_answer(\n",
        "                question_output.last_hidden_state,\n",
        "                question.attention_mask,\n",
        "                answer.input_ids,\n",
        "                answer.attention_mask,\n",
        "                k,\n",
        "            )  # (bsz, 128), (bsz, 128)\n",
        "            return topk_ids, topk_probs\n",
        "\n",
        "    def rank_answer(self, question_states, question_atts, answer_ids, answer_atts, k):\n",
        "        \"\"\"\n",
        "        question_states: (bsz, Lq, d)\n",
        "        answer_ids: answer input id after tokenization, (#answers, La)\n",
        "        \"\"\"\n",
        "        num_ques = question_states.size(0)\n",
        "        start_ids = answer_ids[0, 0].repeat(num_ques, 1)  # bos token\n",
        "\n",
        "        start_output = self.text_decoder(\n",
        "            start_ids,\n",
        "            encoder_hidden_states=question_states,\n",
        "            encoder_attention_mask=question_atts,\n",
        "            return_dict=True,\n",
        "            reduction=\"none\",\n",
        "        )\n",
        "        logits = start_output.logits[:, 0, :]  # first token's logit\n",
        "\n",
        "        # topk_probs: top-k probability\n",
        "        # topk_ids: [num_question, k]\n",
        "        answer_first_token = answer_ids[:, 1]\n",
        "        prob_first_token = F.softmax(logits, dim=1).index_select(\n",
        "            dim=1, index=answer_first_token\n",
        "        )\n",
        "        topk_probs, topk_ids = prob_first_token.topk(k, dim=1)\n",
        "\n",
        "        # answer input: [num_question*k, answer_len]\n",
        "        input_ids = []\n",
        "        input_atts = []\n",
        "        for b, topk_id in enumerate(topk_ids):\n",
        "            input_ids.append(answer_ids.index_select(dim=0, index=topk_id))\n",
        "            input_atts.append(answer_atts.index_select(dim=0, index=topk_id))\n",
        "        input_ids = torch.cat(input_ids, dim=0)\n",
        "        input_atts = torch.cat(input_atts, dim=0)\n",
        "\n",
        "        targets_ids = input_ids.masked_fill(input_ids == self.tokenizer.pad_token_id, -100)\n",
        "\n",
        "        # repeat encoder's output for top-k answers\n",
        "        question_states = tile(question_states, 0, k)\n",
        "        question_atts = tile(question_atts, 0, k)\n",
        "\n",
        "        output = self.text_decoder(\n",
        "            input_ids,\n",
        "            attention_mask=input_atts,\n",
        "            encoder_hidden_states=question_states,\n",
        "            encoder_attention_mask=question_atts,\n",
        "            labels=targets_ids,\n",
        "            return_dict=True,\n",
        "            reduction=\"none\",\n",
        "        )\n",
        "\n",
        "        answer_loss = output.loss\n",
        "        answer_loss = answer_loss.view(input_ids.size(0), -1)\n",
        "\n",
        "        # topk_prob: first token probability\n",
        "        topk_probs = topk_probs.view(-1, 1)\n",
        "        log_probs = torch.cat([topk_probs.log(), -answer_loss], dim=1)\n",
        "\n",
        "        # re-calculate log probabilities for the answer sequences using chain rule\n",
        "        log_probs_sum = log_probs.sum(1)\n",
        "        log_probs_sum = log_probs_sum.view(num_ques, k)\n",
        "\n",
        "        topk_probs = F.softmax(log_probs_sum, dim=-1)\n",
        "        # get top-k after re-ranking\n",
        "        topk_probs, rerank_id = topk_probs.topk(k, dim=1)\n",
        "        topk_ids = torch.gather(topk_ids, 1, rerank_id)\n",
        "\n",
        "        return topk_ids, topk_probs"
      ],
      "metadata": {
        "id": "v_9cyBm-_L7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### build vit"
      ],
      "metadata": {
        "id": "kQj3j529Rebm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "from functools import partial\n",
        "\n",
        "from timm.models.layers import drop_path, to_2tuple, trunc_normal_\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def _cfg(url='', **kwargs):\n",
        "    return {\n",
        "        'url': url,\n",
        "        'num_classes': 400, 'input_size': (3, 224, 224), 'pool_size': None,\n",
        "        'crop_pct': .9, 'interpolation': 'bicubic',\n",
        "        'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5),\n",
        "        **kwargs\n",
        "    }\n",
        "\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return 'p={}'.format(self.drop_prob)\n",
        "\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(\n",
        "            self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n",
        "            proj_drop=0., attn_head_dim=None):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        if attn_head_dim is not None:\n",
        "            head_dim = attn_head_dim\n",
        "        all_head_dim = head_dim * self.num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n",
        "        if qkv_bias:\n",
        "            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
        "            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
        "        else:\n",
        "            self.q_bias = None\n",
        "            self.v_bias = None\n",
        "\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(all_head_dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv_bias = None\n",
        "        if self.q_bias is not None:\n",
        "            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n",
        "        # qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n",
        "        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
        "\n",
        "        q = q * self.scale\n",
        "        attn = (q @ k.transpose(-2, -1))\n",
        "\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
        "                 attn_head_dim=None):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "            attn_drop=attn_drop, proj_drop=drop, attn_head_dim=attn_head_dim)\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "        if init_values > 0:\n",
        "            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
        "            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
        "        else:\n",
        "            self.gamma_1, self.gamma_2 = None, None\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.gamma_1 is None:\n",
        "            x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        else:\n",
        "            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))\n",
        "            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, num_frames=16, tubelet_size=2):\n",
        "        super().__init__()\n",
        "        img_size = to_2tuple(img_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        self.tubelet_size = int(tubelet_size)\n",
        "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0]) * (num_frames // self.tubelet_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "        self.proj = nn.Conv3d(\n",
        "            in_channels=in_chans, out_channels=embed_dim,\n",
        "            kernel_size=(self.tubelet_size, patch_size[0], patch_size[1]),\n",
        "            stride=(self.tubelet_size, patch_size[0], patch_size[1])\n",
        "        )\n",
        "        logger.info(f'Num of patches: {num_patches}')\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        B, C, T, H, W = x.shape\n",
        "        # FIXME look at relaxing size constraints\n",
        "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
        "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "# sin-cos position encoding\n",
        "# https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/Models.py#L31\n",
        "def get_sinusoid_encoding_table(n_position, d_hid, ckpt_num_frame=-1, cur_frame=12):\n",
        "    ''' Sinusoid position encoding table '''\n",
        "    # TODO: make it with torch instead of numpy\n",
        "    def get_position_angle_vec(position):\n",
        "        return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n",
        "\n",
        "    if ckpt_num_frame != -1 and ckpt_num_frame != cur_frame:\n",
        "        logger.info(f\"Interpolate position embedding\")\n",
        "        logger.info(f\"Testing frame: {cur_frame}\")\n",
        "        logger.info(f\"Checkpoint frame: {ckpt_num_frame}\")\n",
        "\n",
        "        T = ckpt_num_frame # checkpoint frame\n",
        "        new_T = cur_frame # testing frame\n",
        "        n_position = n_position // new_T * T # generate checkpoint position embedding\n",
        "        sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n",
        "        sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2]) # dim 2i\n",
        "        sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2]) # dim 2i+1\n",
        "        sinusoid_table = torch.tensor(sinusoid_table, dtype=torch.float, requires_grad=False).unsqueeze(0)\n",
        "        # interpolate\n",
        "        P = int((n_position // T) ** 0.5)\n",
        "        C = d_hid\n",
        "        sinusoid_table = sinusoid_table.reshape(-1, T, P, P, C)\n",
        "        sinusoid_table = sinusoid_table.permute(0, 2, 3, 4, 1).reshape(-1, C, T)  # BHW, C, T\n",
        "        sinusoid_table = torch.nn.functional.interpolate(sinusoid_table, size=new_T, mode='linear')\n",
        "        sinusoid_table = sinusoid_table.reshape(1, P, P, C, new_T).permute(0, 4, 1, 2, 3) # B, T, H, W, C\n",
        "        sinusoid_table = sinusoid_table.flatten(1, 3)\n",
        "        return sinusoid_table\n",
        "    else:\n",
        "        sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n",
        "        sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2]) # dim 2i\n",
        "        sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2]) # dim 2i+1\n",
        "        return torch.tensor(sinusoid_table, dtype=torch.float, requires_grad=False).unsqueeze(0)\n",
        "\n",
        "\n",
        "class PretrainVisionTransformerEncoder(nn.Module):\n",
        "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, depth=12,\n",
        "                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "                 drop_path_rate=0., norm_layer=nn.LayerNorm, init_values=None, num_frames=8, tubelet_size=1,\n",
        "                 use_learnable_pos_emb=False, clip_return_layer=1, clip_student_return_interval=1,\n",
        "                 use_checkpoint=False, checkpoint_num=0, ckpt_num_frame=-1):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
        "        self.patch_embed = PatchEmbed(\n",
        "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
        "            num_frames=num_frames, tubelet_size=tubelet_size\n",
        "        )\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.checkpoint_num = checkpoint_num\n",
        "        logger.info(f\"Use checkpoint: {use_checkpoint}\")\n",
        "        logger.info(f\"Checkpoint number: {checkpoint_num}\")\n",
        "        self.return_index = []\n",
        "        for i in range(clip_return_layer):\n",
        "            self.return_index.append(depth - int(i * clip_student_return_interval) - 1)\n",
        "        logger.info(f\"Student return index: {self.return_index}\")\n",
        "\n",
        "        # TODO: Add the cls token\n",
        "        if use_learnable_pos_emb:\n",
        "            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "            self.img_pos_embed = nn.Parameter(torch.zeros(1, num_patches//(num_frames//tubelet_size) + 1, embed_dim))\n",
        "        else:\n",
        "            # sine-cosine positional embeddings\n",
        "            self.pos_embed = get_sinusoid_encoding_table(num_patches, embed_dim, ckpt_num_frame=ckpt_num_frame, cur_frame=num_frames//tubelet_size)\n",
        "            self.img_pos_embed = get_sinusoid_encoding_table(num_patches//(num_frames//tubelet_size), embed_dim)\n",
        "\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n",
        "                init_values=init_values)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "        if use_learnable_pos_emb:\n",
        "            trunc_normal_(self.pos_embed, std=.02)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'pos_embed', 'cls_token'}\n",
        "\n",
        "    def forward_features(self, x, mask=None, use_image=False):\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        if use_image:\n",
        "            x = x + self.img_pos_embed.type_as(x).to(x.device).clone().detach()\n",
        "        else:\n",
        "            x = x + self.pos_embed.type_as(x).to(x.device).clone().detach()\n",
        "\n",
        "        B, _, C = x.shape\n",
        "        if mask is not None:\n",
        "            x_vis = x[~mask].reshape(B, -1, C) # ~mask means visible\n",
        "        else:\n",
        "            x_vis = x\n",
        "\n",
        "        x_clip_vis = []\n",
        "        for idx, blk in enumerate(self.blocks):\n",
        "            if self.use_checkpoint and idx < self.checkpoint_num:\n",
        "                x_vis = checkpoint.checkpoint(blk, x_vis)\n",
        "            else:\n",
        "                x_vis = blk(x_vis)\n",
        "            if idx in self.return_index:\n",
        "                x_clip_vis.append(x_vis)\n",
        "\n",
        "        x_vis = self.norm(x_vis)\n",
        "        if len(x_clip_vis) > 0:\n",
        "            x_clip_vis = self.norm(torch.stack(x_clip_vis))\n",
        "        return x_vis, x_clip_vis\n",
        "\n",
        "    def forward(self, x, mask=None, use_image=False):\n",
        "        x_vis, x_clip_vis = self.forward_features(x, mask, use_image)\n",
        "        return x_vis, x_clip_vis\n",
        "\n",
        "\n",
        "class Linear_Decoder(nn.Module):\n",
        "    def __init__(self, output_dim=768, embed_dim=768):\n",
        "        super().__init__()\n",
        "\n",
        "        self.head = nn.Linear(embed_dim, output_dim)\n",
        "        self.norm = nn.LayerNorm(output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(self.head(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PretrainVisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 img_size=224,\n",
        "                 patch_size=16,\n",
        "                 encoder_in_chans=3,\n",
        "                 encoder_embed_dim=768,\n",
        "                 encoder_depth=12,\n",
        "                 encoder_num_heads=12,\n",
        "                 mlp_ratio=4.,\n",
        "                 qkv_bias=True,\n",
        "                 qk_scale=None,\n",
        "                 drop_rate=0.,\n",
        "                 attn_drop_rate=0.,\n",
        "                 drop_path_rate=0.,\n",
        "                 norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
        "                 init_values=0.,\n",
        "                 use_learnable_pos_emb=False,\n",
        "                 num_frames=8,\n",
        "                 tubelet_size=1,\n",
        "                 use_checkpoint=False,\n",
        "                 checkpoint_num=0,\n",
        "                 # clip,\n",
        "                 clip_decoder_embed_dim=768,\n",
        "                 clip_output_dim=512,\n",
        "                 clip_return_layer=6,\n",
        "                 clip_student_return_interval=1,\n",
        "                 add_pool_norm=True,\n",
        "                 ckpt_num_frame=-1\n",
        "                ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = PretrainVisionTransformerEncoder(\n",
        "            img_size=img_size,\n",
        "            patch_size=patch_size,\n",
        "            in_chans=encoder_in_chans,\n",
        "            embed_dim=encoder_embed_dim,\n",
        "            depth=encoder_depth,\n",
        "            num_heads=encoder_num_heads,\n",
        "            mlp_ratio=mlp_ratio,\n",
        "            qkv_bias=qkv_bias,\n",
        "            qk_scale=qk_scale,\n",
        "            drop_rate=drop_rate,\n",
        "            attn_drop_rate=attn_drop_rate,\n",
        "            drop_path_rate=drop_path_rate,\n",
        "            norm_layer=norm_layer,\n",
        "            init_values=init_values,\n",
        "            num_frames=num_frames,\n",
        "            tubelet_size=tubelet_size,\n",
        "            use_learnable_pos_emb=use_learnable_pos_emb,\n",
        "            clip_return_layer=clip_return_layer,\n",
        "            clip_student_return_interval=clip_student_return_interval,\n",
        "            use_checkpoint=use_checkpoint,\n",
        "            checkpoint_num=checkpoint_num,\n",
        "            ckpt_num_frame=ckpt_num_frame,\n",
        "        )\n",
        "\n",
        "        # CLIP decoder\n",
        "        self.clip_decoder = nn.ModuleList([\n",
        "            Linear_Decoder(\n",
        "                output_dim=clip_output_dim,\n",
        "                embed_dim=clip_decoder_embed_dim,\n",
        "            ) for _ in range(clip_return_layer)\n",
        "        ])\n",
        "\n",
        "        self.clip_pos_embed = get_sinusoid_encoding_table(\n",
        "            self.encoder.patch_embed.num_patches,\n",
        "            clip_decoder_embed_dim,\n",
        "            ckpt_num_frame=ckpt_num_frame,\n",
        "            cur_frame=num_frames//tubelet_size\n",
        "        )\n",
        "        self.clip_img_pos_embed = get_sinusoid_encoding_table(\n",
        "            self.encoder.patch_embed.num_patches // (num_frames // tubelet_size),\n",
        "            clip_decoder_embed_dim\n",
        "        )\n",
        "\n",
        "        self.add_pool_norm = add_pool_norm\n",
        "        if add_pool_norm:\n",
        "            self.pool_norm = norm_layer(encoder_embed_dim)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'pos_embed', 'cls_token', 'clip_pos_embed'}\n",
        "\n",
        "    def forward(self, x, mask=None, use_image=False, keep_temporal=False):\n",
        "        T = x.shape[2]\n",
        "        x_vis, x_clip_vis = self.encoder(x, mask, use_image) # [B, N_vis, C_e]\n",
        "\n",
        "        if mask is not None and len(x_clip_vis) > 0:\n",
        "            # align CLIP\n",
        "            K, B, _, C_CLIP = x_clip_vis.shape\n",
        "            if use_image:\n",
        "                expand_clip_pos_embed = self.clip_img_pos_embed.repeat(B, 1, 1).type_as(x).to(x.device).clone().detach()\n",
        "            else:\n",
        "                expand_clip_pos_embed = self.clip_pos_embed.repeat(B, 1, 1).type_as(x).to(x.device).clone().detach()\n",
        "            clip_pos_emd_vis = expand_clip_pos_embed[~mask].view(B, -1, C_CLIP).unsqueeze(0).repeat(K, 1, 1, 1)\n",
        "            x_clip_full = x_clip_vis + clip_pos_emd_vis # [K, B, N, C_d_clip]\n",
        "\n",
        "            x_clip = []\n",
        "            for idx, clip_decoder in enumerate(self.clip_decoder):\n",
        "                x_clip.append(clip_decoder(x_clip_full[idx]))\n",
        "            x_clip = torch.stack(x_clip) # align and normalize\n",
        "        else:\n",
        "            x_clip = None\n",
        "\n",
        "        if self.add_pool_norm:\n",
        "            if keep_temporal:\n",
        "                B, _, C_CLIP = x_vis.shape\n",
        "                x_pool_vis = self.pool_norm(x_vis.view(B, T, -1, C_CLIP).mean(2))\n",
        "            else:\n",
        "                x_pool_vis = self.pool_norm(x_vis.mean(1, keepdim=True))\n",
        "\n",
        "            return x_vis, x_pool_vis, x_clip\n",
        "        else:\n",
        "            return x_vis, x_clip"
      ],
      "metadata": {
        "id": "0qeOVqyYrQGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vit(config, add_pool_norm=True):\n",
        "    model = PretrainVisionTransformer(\n",
        "        #img_size = 224,\n",
        "        # img_size=config.vision_encoder.img_size,\n",
        "        # patch_size=config.vision_encoder.patch_size,\n",
        "        # encoder_embed_dim=config.vision_encoder.encoder_embed_dim,\n",
        "        # encoder_depth=config.vision_encoder.encoder_depth,\n",
        "        # encoder_num_heads=config.vision_encoder.encoder_num_heads,\n",
        "        # drop_path_rate=config.vision_encoder.drop_path_rate,\n",
        "        # num_frames=config.vision_encoder.num_frames,\n",
        "        # tubelet_size=config.vision_encoder.tubelet_size,\n",
        "        # use_checkpoint=config.vision_encoder.use_checkpoint,\n",
        "        # checkpoint_num=config.vision_encoder.checkpoint_num,\n",
        "        # clip_decoder_embed_dim=config.vision_encoder.clip_decoder_embed_dim,\n",
        "        # clip_output_dim=config.vision_encoder.clip_output_dim,\n",
        "        # clip_return_layer=config.vision_encoder.clip_return_layer,\n",
        "        # clip_student_return_interval=config.vision_encoder.clip_student_return_interval,\n",
        "        img_size = config['vision_encoder']['img_size'],\n",
        "        patch_size = config['vision_encoder']['patch_size'],\n",
        "        encoder_embed_dim = config['vision_encoder']['encoder_embed_dim'],\n",
        "        encoder_depth = config['vision_encoder']['encoder_depth'],\n",
        "        encoder_num_heads = config['vision_encoder']['encoder_num_heads'],\n",
        "        drop_path_rate = config['vision_encoder']['drop_path_rate'],\n",
        "        num_frames = config['vision_encoder']['num_frames'],\n",
        "        tubelet_size = config['vision_encoder']['tubelet_size'],\n",
        "        use_checkpoint = config['vision_encoder']['use_checkpoint'],\n",
        "        checkpoint_num = config['vision_encoder']['checkpoint_num'],\n",
        "        clip_decoder_embed_dim = config['vision_encoder']['clip_decoder_embed_dim'],\n",
        "        clip_output_dim = config['vision_encoder']['clip_output_dim'],\n",
        "        clip_return_layer = config['vision_encoder']['clip_return_layer'],\n",
        "        clip_student_return_interval = config['vision_encoder']['clip_student_return_interval'],\n",
        "        add_pool_norm=add_pool_norm,\n",
        "        #ckpt_num_frame=config.vision_encoder.get('ckpt_num_frame', -1)\n",
        "    )\n",
        "    model.default_cfg = _cfg()\n",
        "    if config[\"vision_encoder\"][\"pretrained\"] is not None:\n",
        "        print(f\"Loading pretrained weights from { config['vision_encoder']['pretrained'] }\")\n",
        "        state_dict = torch.load(config[\"vision_encoder\"][\"pretrained\"], map_location='cpu')\n",
        "        model.load_state_dict(state_dict, strict=False)\n",
        "        print(model)\n",
        "    else:\n",
        "        print(\"No pretrained weights!!!\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "5-1P1iV1RhpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "from functools import partial\n",
        "\n",
        "from timm.models.layers import drop_path, to_2tuple, trunc_normal_\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def _cfg(url='', **kwargs):\n",
        "    return {\n",
        "        'url': url,\n",
        "        'num_classes': 400, 'input_size': (3, 224, 224), 'pool_size': None,\n",
        "        'crop_pct': .9, 'interpolation': 'bicubic',\n",
        "        'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5),\n",
        "        **kwargs\n",
        "    }\n",
        "\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return 'p={}'.format(self.drop_prob)\n",
        "\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(\n",
        "            self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n",
        "            proj_drop=0., attn_head_dim=None):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        if attn_head_dim is not None:\n",
        "            head_dim = attn_head_dim\n",
        "        all_head_dim = head_dim * self.num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n",
        "        if qkv_bias:\n",
        "            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
        "            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
        "        else:\n",
        "            self.q_bias = None\n",
        "            self.v_bias = None\n",
        "\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(all_head_dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv_bias = None\n",
        "        if self.q_bias is not None:\n",
        "            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n",
        "        # qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n",
        "        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
        "\n",
        "        q = q * self.scale\n",
        "        attn = (q @ k.transpose(-2, -1))\n",
        "\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
        "                 attn_head_dim=None):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "            attn_drop=attn_drop, proj_drop=drop, attn_head_dim=attn_head_dim)\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "        if init_values > 0:\n",
        "            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
        "            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
        "        else:\n",
        "            self.gamma_1, self.gamma_2 = None, None\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.gamma_1 is None:\n",
        "            x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        else:\n",
        "            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))\n",
        "            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, num_frames=16, tubelet_size=2):\n",
        "        super().__init__()\n",
        "        img_size = to_2tuple(img_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        self.tubelet_size = int(tubelet_size)\n",
        "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0]) * (num_frames // self.tubelet_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "        self.proj = nn.Conv3d(\n",
        "            in_channels=in_chans, out_channels=embed_dim,\n",
        "            kernel_size=(self.tubelet_size, patch_size[0], patch_size[1]),\n",
        "            stride=(self.tubelet_size, patch_size[0], patch_size[1])\n",
        "        )\n",
        "        logger.info(f'Num of patches: {num_patches}')\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        B, C, T, H, W = x.shape\n",
        "        # FIXME look at relaxing size constraints\n",
        "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
        "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "# sin-cos position encoding\n",
        "# https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/Models.py#L31\n",
        "def get_sinusoid_encoding_table(n_position, d_hid, ckpt_num_frame=-1, cur_frame=12):\n",
        "    ''' Sinusoid position encoding table '''\n",
        "    # TODO: make it with torch instead of numpy\n",
        "    def get_position_angle_vec(position):\n",
        "        return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n",
        "\n",
        "    if ckpt_num_frame != -1 and ckpt_num_frame != cur_frame:\n",
        "        logger.info(f\"Interpolate position embedding\")\n",
        "        logger.info(f\"Testing frame: {cur_frame}\")\n",
        "        logger.info(f\"Checkpoint frame: {ckpt_num_frame}\")\n",
        "\n",
        "        T = ckpt_num_frame # checkpoint frame\n",
        "        new_T = cur_frame # testing frame\n",
        "        n_position = n_position // new_T * T # generate checkpoint position embedding\n",
        "        sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n",
        "        sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2]) # dim 2i\n",
        "        sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2]) # dim 2i+1\n",
        "        sinusoid_table = torch.tensor(sinusoid_table, dtype=torch.float, requires_grad=False).unsqueeze(0)\n",
        "        # interpolate\n",
        "        P = int((n_position // T) ** 0.5)\n",
        "        C = d_hid\n",
        "        sinusoid_table = sinusoid_table.reshape(-1, T, P, P, C)\n",
        "        sinusoid_table = sinusoid_table.permute(0, 2, 3, 4, 1).reshape(-1, C, T)  # BHW, C, T\n",
        "        sinusoid_table = torch.nn.functional.interpolate(sinusoid_table, size=new_T, mode='linear')\n",
        "        sinusoid_table = sinusoid_table.reshape(1, P, P, C, new_T).permute(0, 4, 1, 2, 3) # B, T, H, W, C\n",
        "        sinusoid_table = sinusoid_table.flatten(1, 3)\n",
        "        return sinusoid_table\n",
        "    else:\n",
        "        sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n",
        "        sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2]) # dim 2i\n",
        "        sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2]) # dim 2i+1\n",
        "        return torch.tensor(sinusoid_table, dtype=torch.float, requires_grad=False).unsqueeze(0)\n",
        "\n",
        "\n",
        "class PretrainVisionTransformerEncoder(nn.Module):\n",
        "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, depth=12,\n",
        "                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "                 drop_path_rate=0., norm_layer=nn.LayerNorm, init_values=None, num_frames=8, tubelet_size=1,\n",
        "                 use_learnable_pos_emb=False, clip_return_layer=1, clip_student_return_interval=1,\n",
        "                 use_checkpoint=False, checkpoint_num=0, ckpt_num_frame=-1):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
        "        self.patch_embed = PatchEmbed(\n",
        "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
        "            num_frames=num_frames, tubelet_size=tubelet_size\n",
        "        )\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.checkpoint_num = checkpoint_num\n",
        "        logger.info(f\"Use checkpoint: {use_checkpoint}\")\n",
        "        logger.info(f\"Checkpoint number: {checkpoint_num}\")\n",
        "        self.return_index = []\n",
        "        for i in range(clip_return_layer):\n",
        "            self.return_index.append(depth - int(i * clip_student_return_interval) - 1)\n",
        "        logger.info(f\"Student return index: {self.return_index}\")\n",
        "\n",
        "        # TODO: Add the cls token\n",
        "        if use_learnable_pos_emb:\n",
        "            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "            self.img_pos_embed = nn.Parameter(torch.zeros(1, num_patches//(num_frames//tubelet_size) + 1, embed_dim))\n",
        "        else:\n",
        "            # sine-cosine positional embeddings\n",
        "            self.pos_embed = get_sinusoid_encoding_table(num_patches, embed_dim, ckpt_num_frame=ckpt_num_frame, cur_frame=num_frames//tubelet_size)\n",
        "            self.img_pos_embed = get_sinusoid_encoding_table(num_patches//(num_frames//tubelet_size), embed_dim)\n",
        "\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n",
        "                init_values=init_values)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "        if use_learnable_pos_emb:\n",
        "            trunc_normal_(self.pos_embed, std=.02)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'pos_embed', 'cls_token'}\n",
        "\n",
        "    def forward_features(self, x, mask=None, use_image=False):\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        if use_image:\n",
        "            x = x + self.img_pos_embed.type_as(x).to(x.device).clone().detach()\n",
        "        else:\n",
        "            x = x + self.pos_embed.type_as(x).to(x.device).clone().detach()\n",
        "\n",
        "        B, _, C = x.shape\n",
        "        if mask is not None:\n",
        "            x_vis = x[~mask].reshape(B, -1, C) # ~mask means visible\n",
        "        else:\n",
        "            x_vis = x\n",
        "\n",
        "        x_clip_vis = []\n",
        "        for idx, blk in enumerate(self.blocks):\n",
        "            if self.use_checkpoint and idx < self.checkpoint_num:\n",
        "                x_vis = checkpoint.checkpoint(blk, x_vis)\n",
        "            else:\n",
        "                x_vis = blk(x_vis)\n",
        "            if idx in self.return_index:\n",
        "                x_clip_vis.append(x_vis)\n",
        "\n",
        "        x_vis = self.norm(x_vis)\n",
        "        if len(x_clip_vis) > 0:\n",
        "            x_clip_vis = self.norm(torch.stack(x_clip_vis))\n",
        "        return x_vis, x_clip_vis\n",
        "\n",
        "    def forward(self, x, mask=None, use_image=False):\n",
        "        x_vis, x_clip_vis = self.forward_features(x, mask, use_image)\n",
        "        return x_vis, x_clip_vis\n",
        "\n",
        "\n",
        "class Linear_Decoder(nn.Module):\n",
        "    def __init__(self, output_dim=768, embed_dim=768):\n",
        "        super().__init__()\n",
        "\n",
        "        self.head = nn.Linear(embed_dim, output_dim)\n",
        "        self.norm = nn.LayerNorm(output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(self.head(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Q7risEKARuKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PretrainVisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 img_size=224,\n",
        "                 patch_size=16,\n",
        "                 encoder_in_chans=3,\n",
        "                 encoder_embed_dim=768,\n",
        "                 encoder_depth=12,\n",
        "                 encoder_num_heads=12,\n",
        "                 mlp_ratio=4.,\n",
        "                 qkv_bias=True,\n",
        "                 qk_scale=None,\n",
        "                 drop_rate=0.,\n",
        "                 attn_drop_rate=0.,\n",
        "                 drop_path_rate=0.,\n",
        "                 norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
        "                 init_values=0.,\n",
        "                 use_learnable_pos_emb=False,\n",
        "                 num_frames=8,\n",
        "                 tubelet_size=1,\n",
        "                 use_checkpoint=False,\n",
        "                 checkpoint_num=0,\n",
        "                 # clip,\n",
        "                 clip_decoder_embed_dim=768,\n",
        "                 clip_output_dim=512,\n",
        "                 clip_return_layer=6,\n",
        "                 clip_student_return_interval=1,\n",
        "                 add_pool_norm=True,\n",
        "                 ckpt_num_frame=-1\n",
        "                ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = PretrainVisionTransformerEncoder(\n",
        "            img_size=img_size,\n",
        "            patch_size=patch_size,\n",
        "            in_chans=encoder_in_chans,\n",
        "            embed_dim=encoder_embed_dim,\n",
        "            depth=encoder_depth,\n",
        "            num_heads=encoder_num_heads,\n",
        "            mlp_ratio=mlp_ratio,\n",
        "            qkv_bias=qkv_bias,\n",
        "            qk_scale=qk_scale,\n",
        "            drop_rate=drop_rate,\n",
        "            attn_drop_rate=attn_drop_rate,\n",
        "            drop_path_rate=drop_path_rate,\n",
        "            norm_layer=norm_layer,\n",
        "            init_values=init_values,\n",
        "            num_frames=num_frames,\n",
        "            tubelet_size=tubelet_size,\n",
        "            use_learnable_pos_emb=use_learnable_pos_emb,\n",
        "            clip_return_layer=clip_return_layer,\n",
        "            clip_student_return_interval=clip_student_return_interval,\n",
        "            use_checkpoint=use_checkpoint,\n",
        "            checkpoint_num=checkpoint_num,\n",
        "            ckpt_num_frame=ckpt_num_frame,\n",
        "        )\n",
        "\n",
        "        # CLIP decoder\n",
        "        self.clip_decoder = nn.ModuleList([\n",
        "            Linear_Decoder(\n",
        "                output_dim=clip_output_dim,\n",
        "                embed_dim=clip_decoder_embed_dim,\n",
        "            ) for _ in range(clip_return_layer)\n",
        "        ])\n",
        "\n",
        "        self.clip_pos_embed = get_sinusoid_encoding_table(\n",
        "            self.encoder.patch_embed.num_patches,\n",
        "            clip_decoder_embed_dim,\n",
        "            ckpt_num_frame=ckpt_num_frame,\n",
        "            cur_frame=num_frames//tubelet_size\n",
        "        )\n",
        "        self.clip_img_pos_embed = get_sinusoid_encoding_table(\n",
        "            self.encoder.patch_embed.num_patches // (num_frames // tubelet_size),\n",
        "            clip_decoder_embed_dim\n",
        "        )\n",
        "\n",
        "        self.add_pool_norm = add_pool_norm\n",
        "        if add_pool_norm:\n",
        "            self.pool_norm = norm_layer(encoder_embed_dim)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'pos_embed', 'cls_token', 'clip_pos_embed'}\n",
        "\n",
        "    def forward(self, x, mask=None, use_image=False, keep_temporal=False):\n",
        "        T = x.shape[2]\n",
        "        x_vis, x_clip_vis = self.encoder(x, mask, use_image) # [B, N_vis, C_e]\n",
        "\n",
        "        if mask is not None and len(x_clip_vis) > 0:\n",
        "            # align CLIP\n",
        "            K, B, _, C_CLIP = x_clip_vis.shape\n",
        "            if use_image:\n",
        "                expand_clip_pos_embed = self.clip_img_pos_embed.repeat(B, 1, 1).type_as(x).to(x.device).clone().detach()\n",
        "            else:\n",
        "                expand_clip_pos_embed = self.clip_pos_embed.repeat(B, 1, 1).type_as(x).to(x.device).clone().detach()\n",
        "            clip_pos_emd_vis = expand_clip_pos_embed[~mask].view(B, -1, C_CLIP).unsqueeze(0).repeat(K, 1, 1, 1)\n",
        "            x_clip_full = x_clip_vis + clip_pos_emd_vis # [K, B, N, C_d_clip]\n",
        "\n",
        "            x_clip = []\n",
        "            for idx, clip_decoder in enumerate(self.clip_decoder):\n",
        "                x_clip.append(clip_decoder(x_clip_full[idx]))\n",
        "            x_clip = torch.stack(x_clip) # align and normalize\n",
        "        else:\n",
        "            x_clip = None\n",
        "\n",
        "        if self.add_pool_norm:\n",
        "            if keep_temporal:\n",
        "                B, _, C_CLIP = x_vis.shape\n",
        "                x_pool_vis = self.pool_norm(x_vis.view(B, T, -1, C_CLIP).mean(2))\n",
        "            else:\n",
        "                x_pool_vis = self.pool_norm(x_vis.mean(1, keepdim=True))\n",
        "\n",
        "            return x_vis, x_pool_vis, x_clip\n",
        "        else:\n",
        "            return x_vis, x_clip\n"
      ],
      "metadata": {
        "id": "9R-ENCyXRswa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build bert"
      ],
      "metadata": {
        "id": "0Sp5wFpwZIAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from uma_models.backbones.bert.xbert import BertConfig, BertForMaskedLM, BertLMHeadModel, BertModel\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def build_bert(model_config, pretrain, checkpoint):\n",
        "    \"\"\"build text encoder.\n",
        "\n",
        "    Args:\n",
        "        model_config (dict): model config.\n",
        "        pretrain (bool): Whether to do pretrain or finetuning.\n",
        "        checkpoint (bool): whether to do gradient_checkpointing.\n",
        "\n",
        "    Returns: TODO\n",
        "\n",
        "    \"\"\"\n",
        "    #print(model_config)\n",
        "    bert_config = BertConfig.from_json_file(model_config[\"text_encoder\"][\"config\"])\n",
        "    bert_config.encoder_width = model_config[\"vision_encoder\"][\"d_model\"]\n",
        "    bert_config.gradient_checkpointing = checkpoint\n",
        "    bert_config.fusion_layer = model_config[\"text_encoder\"][\"fusion_layer\"]\n",
        "\n",
        "    if not model_config[\"multimodal\"][\"enable\"]:\n",
        "        bert_config.fusion_layer = bert_config.num_hidden_layers\n",
        "\n",
        "    if pretrain:\n",
        "        text_encoder, loading_info = BertForMaskedLM.from_pretrained(\n",
        "            model_config[\"text_encoder\"][\"pretrained\"],\n",
        "            config=bert_config,\n",
        "            output_loading_info=True,\n",
        "        )\n",
        "    else:\n",
        "        text_encoder, loading_info = BertModel.from_pretrained(\n",
        "            model_config[\"text_encoder\"][\"pretrained\"],\n",
        "            config=bert_config,\n",
        "            add_pooling_layer=False,\n",
        "            output_loading_info=True,\n",
        "        )\n",
        "\n",
        "    return text_encoder\n",
        "\n",
        "\n",
        "def build_bert_decoder(model_config, checkpoint):\n",
        "    \"\"\"build text decoder the same as the multimodal encoder.\n",
        "\n",
        "    Args:\n",
        "        model_config (dict): model config.\n",
        "        pretrain (bool): Whether to do pretrain or finetuning.\n",
        "        checkpoint (bool): whether to do gradient_checkpointing.\n",
        "\n",
        "    Returns: TODO\n",
        "\n",
        "    \"\"\"\n",
        "    bert_config = BertConfig.from_json_file(model_config[\"text_encoder\"][\"config\"])\n",
        "    bert_config.encoder_width = model_config[\"vision_encoder\"][\"d_model\"]\n",
        "    bert_config.gradient_checkpointing = checkpoint\n",
        "\n",
        "    bert_config.fusion_layer = 0\n",
        "    bert_config.num_hidden_layers = (\n",
        "        bert_config.num_hidden_layers - model_config[\"text_encoder\"][\"fusion_layer\"]\n",
        "    )\n",
        "\n",
        "    text_decoder, loading_info = BertLMHeadModel.from_pretrained(\n",
        "        model_config[\"text_encoder\"][\"pretrained\"],\n",
        "        config=bert_config,\n",
        "        output_loading_info=True,\n",
        "    )\n",
        "\n",
        "    return text_decoder\n"
      ],
      "metadata": {
        "id": "ihWmrOg5ZLee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resume"
      ],
      "metadata": {
        "id": "uGJ6Um89--DW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import umt_cfg\n",
        "config = umt_cfg"
      ],
      "metadata": {
        "id": "bDopEZs0JZxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config.model = config.model\n",
        "print(config.model.model_cls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "t3vQM-9JAd2W",
        "outputId": "5436b533-0dcb-4c79-9bd6-b7541f9ded4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-181-c560cb09e404>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'model_cls'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config.pretrained_path = './uma_checkpoints/qa_anet_b16_25m.pth'\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "wNSRea6zxPNp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "a738cb1d60964faaba7d6f3a5f665f5e",
            "da9e1072e92c48949814f3be812ce521",
            "ca594cbbc0f249d4bbf16682989cd1ed",
            "217a453177ac4216b888ecaac9eea6bf",
            "ef14200fb01743dc8a9533df44197507",
            "e63022385c514b7cb9cd819ee4f2b402",
            "163d7cc6529c46e48be0084dbb336278",
            "43f297e1fb17443b92652202a61aaa7d",
            "44792c58989b405db23be4771d1db5ca",
            "600c16017e994026ba4d30956fa1740b",
            "6f7f1dc22c774645b347c157a553f404",
            "ced496820a1a4803b086b626f6b9e439",
            "c0449838b4be4c529a2a3fd29d0f174b",
            "4b786ede155a4bd393f04698b08cb8ae",
            "8637da21baa342dbbe09a5b10e84cda3",
            "13f3b292cf1d4f44ac24f116814c01b7",
            "f3a1ebf6a3eb4aa89368b6e46ae04bd0",
            "2dddb634372d42aaab1a6aa4207ba24a",
            "c9b04ee6e02e43ec8f14eeacd8bf5e02",
            "8a588fa60aee4eaa9512175424dbff04",
            "13c533832e1147a99baa214211d1ea93",
            "e8abcebcca784e17a6360c4bca4619f4",
            "288c2513e3824b06bf2af1e6d79cd0e3",
            "a2bc226eb6b74d22bef9e61c3cb40b84",
            "c4a9844ce16c47e9ba64bdc458d3d5e7",
            "8fee7fe5486f48fc964f6b310c3000c6",
            "3bdec310459d4e5fb6675f2bc8dd1f51",
            "3703bcd981e54210b09b4ec878082ef2",
            "a563da5f92ed43d1a4872dea126739ec",
            "9bdb7855e696441b92e3dbce7a24cef9",
            "7b3dd09494474f989c379df1f7fa3d3f",
            "a605278f9057449f94a09728c2d0fcaa",
            "4844d935f117498bbfd0f6eb69ce42eb"
          ]
        },
        "outputId": "7c39887f-283c-4380-93f6-e4be11b892fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a738cb1d60964faaba7d6f3a5f665f5e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ced496820a1a4803b086b626f6b9e439"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "288c2513e3824b06bf2af1e6d79cd0e3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = UMT_QA_(config=config, tokenizer=tokenizer, is_pretrain=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAdHRJwRCAXc",
        "outputId": "30b54ae7-b5d0-4312-fe0e-86cbf8de587c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Build vision_encoder: vit_b16\n",
            "Loading pretrained weights from uma_checkpoints/b16_ptk710_f8_res224.pth\n",
            "PretrainVisionTransformer(\n",
            "  (encoder): PretrainVisionTransformerEncoder(\n",
            "    (patch_embed): PatchEmbed(\n",
            "      (proj): Conv3d(3, 768, kernel_size=(1, 16, 16), stride=(1, 16, 16))\n",
            "    )\n",
            "    (blocks): ModuleList(\n",
            "      (0): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU(approximate='none')\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (1): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath(p=0.0181818176060915)\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU(approximate='none')\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (2): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath(p=0.036363635212183)\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU(approximate='none')\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (3): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath(p=0.05454545468091965)\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU(approximate='none')\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (4): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath(p=0.072727270424366)\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU(approximate='none')\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (5): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath(p=0.09090908616781235)\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU(approximate='none')\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (6): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath(p=0.10909091681241989)\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU(approximate='none')\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (7): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath(p=0.12727272510528564)\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU(approximate='none')\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (8): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath(p=0.1454545557498932)\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU(approximate='none')\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (9): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath(p=0.16363637149333954)\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU(approximate='none')\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (10): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath(p=0.1818181872367859)\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU(approximate='none')\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (11): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath(p=0.20000000298023224)\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU(approximate='none')\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (clip_decoder): ModuleList()\n",
            ")\n",
            "Build text_decoder bert_base\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(torch.device(config.device))\n",
        "model_without_ddp = model"
      ],
      "metadata": {
        "id": "VUhindiob6Sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "raw_answer_list = test_loader.dataset"
      ],
      "metadata": {
        "id": "XQJ6uZ-FcjUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = test_loader.dataset[0]\n",
        "#image, question, question_id = data\n",
        "#use our dataset instead\n",
        "image, candidate_qa, gt_qa, qid, sampled_frames, answer = data\n",
        "video = image.to(device, non_blocking=True)\n",
        "question_input = tokenizer(\n",
        "    #question,\n",
        "    candidate_qa,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=config.max_q_len,\n",
        "    return_tensors=\"pt\",\n",
        ").to(device)\n",
        "topk_ids, topk_probs = model(\n",
        "  video.permute(0,3,1,2).unsqueeze(-1), question_input, question_input, train=False, k=config.evaluation[\"k_test\"]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "NXBRKD1OfS9X",
        "outputId": "e3cd6951-8761-46c0-d893-ee4d9dd5c611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "video 0 dim: torch.Size([8, 480, 270, 3])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-7a73a2c20c05>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m ).to(device)\n\u001b[0;32m---> 14\u001b[0;31m topk_ids, topk_probs = model(\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"k_test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-d958eee3e116>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image, question, answer, k, weights, train)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mweights\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \"\"\"\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mimage_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_vision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"dimension of image(video) embedding: {image_embeds.size()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"image embed: {image_embeds}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-d958eee3e116>\u001b[0m in \u001b[0;36mencode_vision\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# whether save temporal dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mkeep_temporal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vision_encoder\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"keep_temporal\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         vision_embeds, _ = self.vision_encoder(\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-669a693a5197>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask, use_image, keep_temporal)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_temporal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mx_vis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_clip_vis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_image\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [B, N_vis, C_e]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_clip_vis\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-174513ad61b0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask, use_image)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mx_vis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_clip_vis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx_vis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_clip_vis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-174513ad61b0>\u001b[0m in \u001b[0;36mforward_features\u001b[0;34m(self, x, mask, use_image)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_image\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-174513ad61b0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# FIXME look at relaxing size constraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0;34mf\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Input image size (270*1) doesn't match model (224*224)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for n, data in enumerate(test_loader):\n",
        "    #image, question, question_id = data\n",
        "    #use our dataset instead\n",
        "    image, candidate_qa, gt_qa, qid, sampled_frames, answer = data\n",
        "    video = video.to(device, non_blocking=True)\n",
        "    question_input = tokenizer(\n",
        "        #question,\n",
        "        candidates_QApair,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=config.max_q_len,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "    topk_ids, topk_probs = model(\n",
        "      video, question_input, question_input, train=False, k=config.evaluation.k_test\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g5wOr4xueYGg",
        "outputId": "3b7980ec-74a9-49f1-8066-23569fce16ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "video 32 dim: torch.Size([8, 320, 480, 3])\n",
            "video 0 dim: torch.Size([8, 480, 270, 3])\n",
            "video 1 dim: torch.Size([8, 480, 270, 3])\n",
            "video 33 dim: torch.Size([8, 480, 270, 3])\n",
            "video 2 dim: torch.Size([8, 480, 270, 3])\n",
            "video 34 dim: torch.Size([8, 480, 270, 3])\n",
            "video 3 dim: torch.Size([8, 480, 270, 3])\n",
            "video 35 dim: torch.Size([8, 480, 360, 3])\n",
            "video 4 dim: torch.Size([8, 480, 270, 3])\n",
            "video 5 dim: torch.Size([8, 480, 270, 3])\n",
            "video 36 dim: torch.Size([8, 480, 270, 3])\n",
            "video 6 dim: torch.Size([8, 480, 270, 3])\n",
            "video 7 dim: torch.Size([8, 480, 270, 3])\n",
            "video 37 dim: torch.Size([8, 320, 480, 3])\n",
            "video 8 dim: torch.Size([8, 480, 270, 3])\n",
            "video 38 dim: torch.Size([8, 320, 480, 3])\n",
            "video 39 dim: torch.Size([8, 480, 270, 3])\n",
            "video 40 dim: torch.Size([8, 480, 270, 3])\n",
            "video 9 dim: torch.Size([8, 480, 270, 3])\n",
            "video 10 dim: torch.Size([8, 480, 270, 3])\n",
            "video 11 dim: torch.Size([8, 480, 270, 3])\n",
            "video 12 dim: torch.Size([8, 480, 270, 3])\n",
            "video 41 dim: torch.Size([8, 480, 320, 3])\n",
            "video 42 dim: torch.Size([8, 480, 320, 3])\n",
            "video 13 dim: torch.Size([8, 480, 270, 3])\n",
            "video 43 dim: torch.Size([8, 480, 270, 3])\n",
            "video 44 dim: torch.Size([8, 480, 270, 3])\n",
            "video 45 dim: torch.Size([8, 480, 270, 3])\n",
            "video 14 dim: torch.Size([8, 480, 270, 3])\n",
            "video 15 dim: torch.Size([8, 480, 270, 3])\n",
            "video 16 dim: torch.Size([8, 480, 270, 3])\n",
            "video 17 dim: torch.Size([8, 480, 320, 3])\n",
            "video 18 dim: torch.Size([8, 480, 320, 3])\n",
            "video 46 dim: torch.Size([8, 480, 270, 3])\n",
            "video 47 dim: torch.Size([8, 480, 270, 3])\n",
            "video 48 dim: torch.Size([8, 480, 270, 3])\n",
            "video 49 dim: torch.Size([8, 480, 320, 3])\n",
            "video 50 dim: torch.Size([8, 480, 270, 3])\n",
            "video 19 dim: torch.Size([8, 480, 320, 3])\n",
            "video 20 dim: torch.Size([8, 480, 320, 3])\n",
            "video 21 dim: torch.Size([8, 480, 320, 3])\n",
            "video 51 dim: torch.Size([8, 480, 270, 3])\n",
            "video 52 dim: torch.Size([8, 480, 270, 3])\n",
            "video 53 dim: torch.Size([8, 480, 270, 3])\n",
            "video 22 dim: torch.Size([8, 480, 320, 3])\n",
            "video 23 dim: torch.Size([8, 480, 320, 3])\n",
            "video 54 dim: torch.Size([8, 480, 270, 3])\n",
            "video 55 dim: torch.Size([8, 480, 270, 3])\n",
            "video 56 dim: torch.Size([8, 480, 270, 3])\n",
            "video 24 dim: torch.Size([8, 480, 270, 3])\n",
            "video 57 dim: torch.Size([8, 480, 270, 3])\n",
            "video 25 dim: torch.Size([8, 480, 270, 3])\n",
            "video 58 dim: torch.Size([8, 480, 270, 3])\n",
            "video 26 dim: torch.Size([8, 360, 480, 3])\n",
            "video 59 dim: torch.Size([8, 480, 270, 3])\n",
            "video 27 dim: torch.Size([8, 360, 480, 3])\n",
            "video 60 dim: torch.Size([8, 480, 270, 3])\n",
            "video 28 dim: torch.Size([8, 480, 270, 3])\n",
            "video 61 dim: torch.Size([8, 480, 270, 3])\n",
            "video 29 dim: torch.Size([8, 480, 270, 3])\n",
            "video 30 dim: torch.Size([8, 480, 270, 3])\n",
            "video 62 dim: torch.Size([8, 480, 270, 3])\n",
            "video 31 dim: torch.Size([8, 480, 320, 3])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-194-b934d4501a15>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m#image, question, question_id = data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#use our dataset instead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_qa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_qa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"<ipython-input-6-450ca6549e5d>\", line 12, in vqa_collate_fn\n    torch.stack(image_list, dim=0),\nRuntimeError: stack expects each tensor to be equal size, but got [8, 480, 270, 3] at entry 0 and [8, 480, 320, 3] at entry 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "video 63 dim: torch.Size([8, 480, 270, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topk_ids, topk_probs = model(\n",
        "    video, question_input, answer_input, train=False, k=config.evaluation.k_test\n",
        ")"
      ],
      "metadata": {
        "id": "PORZv-PleNjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "    metric_logger = MetricLogger(delimiter=\"  \")\n",
        "    header = \"[evaluation] Generating answers:\"\n",
        "    log_freq = config.log_freq // 2\n",
        "\n",
        "    result = []\n",
        "\n",
        "    raw_answer_list = data_loader.dataset.answer_list #list of answers to video questions\n",
        "    answer_list = [a + \" \" + config.eos for a in raw_answer_list]\n",
        "    answer_input = tokenizer(\n",
        "        answer_list,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=config.max_a_len,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "\n",
        "    logger.info(\"Start generating results.\")\n",
        "\n",
        "    #iterator = metric_logger.log_every(data_loader, log_freq, header)\n",
        "    iterator = data_loader\n",
        "    for n, data in enumerate(iterator):\n",
        "        #image, question, question_id = data\n",
        "        #use our dataset instead\n",
        "        video, candidates_QApair, gt_QApair, answer_text = data\n",
        "        video = video.to(device, non_blocking=True)\n",
        "        question_input = tokenizer(\n",
        "            #question,\n",
        "            candidates_QApair,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=config.max_q_len,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(device)\n",
        "\n",
        "        print(f\"evaluating question input: {candidate_QApair}, {question_input}\")\n",
        "\n",
        "        topk_ids, topk_probs = model(\n",
        "            video, question_input, answer_input, train=False, k=config.evaluation.k_test\n",
        "        )\n",
        "\n",
        "        for ques_id, topk_id, topk_prob in zip(question_id, topk_ids, topk_probs):\n",
        "            ques_id = int(ques_id.item()) if not isinstance(ques_id, str) else ques_id\n",
        "            _, pred = topk_prob.max(dim=0)\n",
        "            result.append({\"question_id\": ques_id, \"answer\": raw_answer_list[topk_id[pred]]})\n",
        "\n",
        "    print(f\"evaluation result: {result}\")\n",
        "    return result"
      ],
      "metadata": {
        "id": "_hYb_RM5cODX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer & scheduler\n",
        "optimizer = create_optimizer(config.optimizer, model)\n",
        "scheduler = create_scheduler(config.scheduler, optimizer)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=config.fp16)\n",
        "\n",
        "start_epoch = 0\n",
        "global_step = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "2nxAvNy2b-Zv",
        "outputId": "cb4f6e7e-7032-4e41-c9f1-4f6d1ec3e217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-186-cbf875f84092>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# optimizer & scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/.shortcut-targets-by-id/1lKO9qUQLnjUauiyqzl6VVs2RkwHidE3c/unmasked teacher/uma_utils/optimizer.py\u001b[0m in \u001b[0;36mcreate_optimizer\u001b[0;34m(args, model, filter_bias_and_bn)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_bias_and_bn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mopt_lower\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m# check for modules that requires different lr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'opt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_vqa_model(\n",
        "    config, model_cls, has_decoder=False, pretrain=False, find_unused_parameters=False\n",
        "):\n",
        "    #logger.info(\"Creating model\")\n",
        "    #config = copy.deepcopy(config)\n",
        "    print(f\"setting up vqa model, with configuration: {config}\")\n",
        "\n",
        "    config.pretrained_path = './uma_checkpoints/qa_anet_b16_25m.pth'\n",
        "    '''\n",
        "    --- Text Encoder ---\n",
        "    TextEncoders[\"bert\"] = dict(\n",
        "    name=\"bert_base\",\n",
        "    pretrained=\"bert-base-uncased\",\n",
        "    config=\"configs/config_bert.json\",\n",
        "    d_model=768,\n",
        "    fusion_layer=9,\n",
        "    )\n",
        "    '''\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    '''\n",
        "    if \"bert\" in config.model[\"text_encoder\"][\"name\"]:\n",
        "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    else:\n",
        "        raise ValueError(f\"Not supported text encoder.\")\n",
        "    '''\n",
        "\n",
        "    # UMT-QA model\n",
        "    model = UMT_QA(config=config, tokenizer=tokenizer, is_pretrain=pretrain)\n",
        "\n",
        "    model = model.to(torch.device(config.device))\n",
        "    model_without_ddp = model\n",
        "\n",
        "    '''\n",
        "    --- For parallel computation ---\n",
        "    if config.distributed:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(\n",
        "            model,\n",
        "            device_ids=[config.gpu],\n",
        "            find_unused_parameters=find_unused_parameters,  # `False` for image-only task\n",
        "        )\n",
        "    '''\n",
        "\n",
        "    # optimizer & scheduler\n",
        "    optimizer = create_optimizer(config.optimizer, model)\n",
        "    scheduler = create_scheduler(config.scheduler, optimizer)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=config.fp16)\n",
        "\n",
        "    start_epoch = 0\n",
        "    global_step = 0\n",
        "\n",
        "    # auto resume the latest checkpoint\n",
        "    if config.get(\"auto_resume\", False):\n",
        "        logger.info(\"Auto resuming\")\n",
        "        model_latest = join(config.output_dir, \"ckpt_latest.pth\")\n",
        "        model_best = join(config.output_dir, \"ckpt_best.pth\")\n",
        "        large_num = -1\n",
        "        for p in os.listdir(config.output_dir):\n",
        "            if 'ckpt' in p:\n",
        "                num = p.split('_')[1].split('.')[0]\n",
        "                if str.isnumeric(num):\n",
        "                    if int(num) > large_num:\n",
        "                        large_num = int(num)\n",
        "        if large_num != -1:\n",
        "            model_latest = join(config.output_dir, f\"ckpt_{large_num:02d}.pth\")\n",
        "        if osp.isfile(model_latest):\n",
        "            config.pretrained_path = model_latest\n",
        "            config.resume = True\n",
        "        elif osp.isfile(model_best):\n",
        "            config.pretrained_path = model_best\n",
        "            config.resume = True\n",
        "        else:\n",
        "            logger.info(f\"Not found checkpoint in {config.output_dir}\")\n",
        "\n",
        "    if osp.isfile(config.pretrained_path):\n",
        "        checkpoint = torch.load(config.pretrained_path, map_location=\"cpu\")\n",
        "        if 'model' in checkpoint.keys():\n",
        "            state_dict = checkpoint[\"model\"]\n",
        "        else:\n",
        "            state_dict = checkpoint\n",
        "\n",
        "        if config.resume:\n",
        "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "            scheduler.load_state_dict(checkpoint[\"scheduler\"])\n",
        "            scaler.load_state_dict(checkpoint[\"scaler\"])\n",
        "            start_epoch = checkpoint[\"epoch\"] + 1\n",
        "            global_step = checkpoint[\"global_step\"]\n",
        "        elif not pretrain:  # downstream init from pretrained ckpt\n",
        "\n",
        "            # interpolate positional embeddings.\n",
        "            if \"vit\" in config.model.vision_encoder.name:\n",
        "                pass\n",
        "            else:\n",
        "                raise ValueError(\n",
        "                    f\" vision encoder: {config.model.vision_encoder.name} not implelented\"\n",
        "                )\n",
        "\n",
        "            # finetuning starts here ->\n",
        "            if not config.evaluate or config.get(\"zero_shot\", False):  # finetuning from a pretarined weights.\n",
        "                for key in list(state_dict.keys()):\n",
        "                    if \"bert\" in key:\n",
        "                        encoder_key = key.replace(\"bert.\", \"\")\n",
        "                        state_dict[encoder_key] = state_dict[key]\n",
        "                        if not has_decoder:\n",
        "                            del state_dict[key]\n",
        "\n",
        "                    # init text decoder as multimodal encoder (last 6 layers of model.text_encoder)\n",
        "                    # only for generation tasks like VQA\n",
        "                    if has_decoder and \"text_encoder\" in key:\n",
        "                        if \"layer\" in key:\n",
        "                            encoder_keys = key.split(\".\")\n",
        "                            layer_num = int(encoder_keys[4])\n",
        "                            if layer_num < config.model.text_encoder.fusion_layer:\n",
        "                                del state_dict[key]\n",
        "                                continue\n",
        "                            else:\n",
        "                                decoder_layer_num = layer_num - config.model.text_encoder.fusion_layer\n",
        "                                encoder_keys[4] = str(decoder_layer_num)\n",
        "                                encoder_key = \".\".join(encoder_keys)\n",
        "                        else:\n",
        "                            encoder_key = key\n",
        "                        decoder_key = encoder_key.replace(\"text_encoder\", \"text_decoder\")\n",
        "                        state_dict[decoder_key] = state_dict[key]\n",
        "                        del state_dict[key]\n",
        "\n",
        "        msg = model_without_ddp.load_state_dict(state_dict, strict=False)\n",
        "        logger.info(msg)\n",
        "        logger.info(f\"Loaded checkpoint from {config.pretrained_path}\")\n",
        "    else:\n",
        "        logger.warning(\"No pretrained checkpoint provided, training from scratch\")\n",
        "\n",
        "    return (\n",
        "        model,\n",
        "        model_without_ddp,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        scaler,\n",
        "        tokenizer,\n",
        "        start_epoch,\n",
        "        global_step,\n",
        "    )"
      ],
      "metadata": {
        "id": "Y54HbQ1SxM5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_result = evaluation(model, test_loader, tokenizer, device, config)"
      ],
      "metadata": {
        "id": "YHZG8xw5HePw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if is_main_process() and config.wandb.enable:\n",
        "    wandb.watch(model)\n",
        "\n",
        "best = 0\n",
        "best_epoch = 0\n",
        "has_gt = config.dataset_name != \"vqa\" or config.test_types[0] == \"minival\"\n",
        "\n",
        "logger.info(\"Start \" + \"evaluation\" if config.evaluate else \"training\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(start_epoch, config.scheduler.epochs):\n",
        "\n",
        "    # ----- train mode (can ignore temporarily) ----- #\n",
        "    if not config.evaluate:\n",
        "        global_step = train(\n",
        "            model,\n",
        "            train_loader,\n",
        "            optimizer,\n",
        "            tokenizer,\n",
        "            epoch,\n",
        "            global_step,\n",
        "            device,\n",
        "            scheduler,\n",
        "            scaler,\n",
        "            config,\n",
        "        )\n",
        "\n",
        "    if config.get('no_test', False) and not config.evaluate:\n",
        "        if is_main_process():\n",
        "            save_obj = {\n",
        "                \"model\": model_without_ddp.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "                \"scheduler\": scheduler.state_dict(),\n",
        "                \"scaler\": scaler.state_dict(),\n",
        "                \"config\": config,\n",
        "                \"epoch\": epoch,\n",
        "                \"global_step\": global_step,\n",
        "            }\n",
        "            torch.save(save_obj, join(config.output_dir, \"ckpt_best.pth\"))\n",
        "            best_epoch = epoch\n",
        "\n",
        "    # ----- evaluate ----- #\n",
        "    else:\n",
        "        with torch.cuda.amp.autocast(enabled=config.fp16):\n",
        "            eval_res = {}\n",
        "            pred_name2file = {}\n",
        "\n",
        "            '''\n",
        "            --- We only need to evaluate vqa task on one dataset ---\n",
        "            for test_name, test_loader in test_name2loaders.items():\n",
        "                if test_name not in config.test_types:\n",
        "                    logger.info(\n",
        "                        f\"Skip eval {test_name} split. All test_types {config.test_types}\"\n",
        "                    )\n",
        "                    continue\n",
        "                logger.info(f\"Evaluating {test_name} split...\")\n",
        "\n",
        "                # evaluate qa result\n",
        "                qa_result = evaluation(model, test_loader, tokenizer, device, config)\n",
        "\n",
        "                # sync/gather results and eval\n",
        "                pred_file, gathered_result = sync_save_result(\n",
        "                    qa_result, config.result_dir, f\"{test_name}_latest\"\n",
        "                )\n",
        "                pred_name2file[test_name] = pred_file\n",
        "                if is_main_process() and has_gt:\n",
        "                    eval_res[test_name] = eval_qa_acc(\n",
        "                        test_loader.dataset.anno_list,\n",
        "                        gathered_result,\n",
        "                        is_vqa=config.dataset_name == \"vqa\",\n",
        "                    )\n",
        "              '''\n",
        "              # evaluate qa result\n",
        "              qa_result = evaluation(model, test_loader, tokenizer, device, config)"
      ],
      "metadata": {
        "id": "Bu7VqlT6quv4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}